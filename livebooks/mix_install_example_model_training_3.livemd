# (breast cancer) Mix.Install

```elixir
Mix.install([
  {:axon, "~> 0.2.0-dev", github: "elixir-nx/axon"},
  {:exla, "~> 0.3.0-dev", github: "elixir-nx/nx", sparse: "exla"},
  {:nx, "~> 0.3.0-dev", github: "elixir-nx/nx", sparse: "nx", override: true},
  {:explorer, "~> 0.2.0"},
  {:kino, "~> 0.6.2"},
  {:kino_vega_lite, "~> 0.1.1"},
  {:req, "~> 0.3.0"},
  {:vega_lite, "~> 0.1.4"}
])
```

## Get the data - Breast Cancer

The dataset for this experiment is the [Haberman Dataset](https://archive.ics.uci.edu/ml/datasets/haberman's+survival). The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer

<!-- livebook:{"break_markdown":true} -->

### Attributes

* Age of patient at time of operation (numerical)
* Patient's year of operation (year - 1900, numerical)
* Number of positive axillary nodes detected (numerical)
* Survival status (class attribute)
  * 1 = the patient survived 5 years or longer
  * 0 = the patient died within 5 year

```elixir
# df = Explorer.Datasets.iris()
alias Explorer.DataFrame
alias Explorer.Series

df = DataFrame.from_csv!("Projects/elixir_conf/data/haberman.csv")
```

### Shape

```elixir
DataFrame.shape(df)
```

## Size the data

Determine how many rows are in the dataset. Use that number to determine the size needed for training and testing so that 85% of examples are for training and 15% for testing.

```elixir
sample_size = DataFrame.n_rows(df) |> IO.inspect(label: "Number of rows")

train_size = ceil(0.85 * sample_size) |> IO.inspect(label: "Training")
test_size = (sample_size - train_size) |> IO.inspect(label: "Testing")

:ok
```

Use those percentages to slice the data into training and testing sets

```elixir
train_df = DataFrame.slice(df, 0, train_size)
test_df = DataFrame.slice(df, 0, test_size)

:ok
```

## Split Features from Targets

Split both the training and testing sets into two sets: one set of features and other set of targets because Axon expects tuples of {features, targets} to train a model. The target in this class is the "class" column, the 1 if the patient survived 5 years or longer or 0 if they did not. The rest of the columns are features.

```elixir
train_df_inputs =
  DataFrame.select(train_df, &(&1 == "class"), :drop) |> IO.inspect(label: "features")

train_df_targets =
  DataFrame.select(train_df, &(&1 == "class"), :keep) |> IO.inspect(label: "targets")

test_df_inputs = DataFrame.select(test_df, &(&1 == "class"), :drop)
test_df_targets = DataFrame.select(test_df, &(&1 == "class"), :keep)

:ok
```

### Convert the sets to tensors

<!-- livebook:{"break_markdown":true} -->

The training and testing tests are still Explorer DataFrames, but Axon expects Nx Tensors. Convert the sets into Nx tensors.

```elixir
to_tensor = fn data_frame ->
  data_frame
  |> DataFrame.names()
  |> Enum.map(
    &(Series.to_tensor(data_frame[&1])
      |> Nx.new_axis(-1))
  )
  |> Nx.concatenate(axis: 1)
end

train_inputs = to_tensor.(train_df_inputs) |> IO.inspect(label: "features")
train_targets = to_tensor.(train_df_targets) |> IO.inspect(label: "targets")

test_inputs = to_tensor.(test_df_inputs)
test_targets = to_tensor.(test_df_targets)

:ok
```

### Convert to lists of tensors

<!-- livebook:{"break_markdown":true} -->

Axon expectsd small batches for training. Nx `to_bached/3` converts a tensor to a stream of tensor batches. From the [docs](https://hexdocs.pm/nx/Nx.html#to_batched_list/3):

> The first dimension (axis 0) is divided by `batch_size`. In case the dimension cannot be evenly divided by `batch_size`, you may specify what to do with leftover data using `:leftover`. `:leftover` must be one of `:repeat` or `:discard`. `:repeat` repeats the first `n` values to make the last batch match the desired batch size. `:discard` discards excess elements.

The training set has 261 rows and the testing set has 45. We will set the `batch_size` to 15 and the `leftover` to `:repeat` since our dataset is small.

```elixir
# Training set size: 261; testing set size: 45
batch_size = 15

train_batch_inputs =
  Nx.to_batched(train_inputs, batch_size, leftover: :repeat)
  |> IO.inspect(label: "batched features")

train_batch_targets =
  Nx.to_batched(train_targets, batch_size, leftover: :repeat)
  |> IO.inspect(label: "batched targets")

test_batch_inputs = Nx.to_batched(test_inputs, batch_size, leftover: :repeat)
test_batch_targets = Nx.to_batched(test_targets, batch_size, leftover: :repeat)

:ok
```

Zip the batched features with the batched targets for each set, training and testing.

```elixir
train_batch = Stream.zip(train_batch_inputs, train_batch_targets)
test_batch = Stream.zip(test_batch_inputs, test_batch_targets)

:ok
```

### Normalize

<!-- livebook:{"break_markdown":true} -->

Normalize the input data. Ideally, each column should be a value between 1 and 0. This increases the performance of the model. The simpliest way to achieve normalization is to divide each feature by the maximum value of the feature.

For this dataset we need to normalize the 3 feature columns. The target column is already a 1 or 0.

```elixir
train_max =
  Nx.reduce_max(train_inputs, axes: [0], keep_axes: true)
  |> IO.inspect(label: "max value of each feature")

normalize = fn {batch, target} ->
  normalized_feature = Nx.divide(batch, train_max)
  {normalized_feature, target}
end

training_data =
  train_batch
  |> Stream.map(&Nx.Defn.jit(normalize, [&1]))

:ok
```

## Model

### Define the Model

The model trained here is a feed-forward neural network. You can choose among many established models, but this one is good to classify our features as predictive of surgery survival or not.

The dataset is relatively small, with a very small set of input features. This means the model can do well with a minimum number of intermediate layers. Another way to think of this is the number of neuronal layers and the number of neurons comprising each layer.

For most problems, good performance can be achieved by determining the hidden layer using  two rules:

1. the number of hidden layers equals one
2. the number of neurons in that layer is the mean of the neurons in the input and output layers

The size of the input layer is equal to the number of features (columns) in your data. Some NN configurations add one additional node for a bias term. In our case, the number is *4*.The size of the output layer is typically 1.

* 3 hidden layers with a hidden size of 2

This is a starting point. Chaning the hidden size or activations will produce different outcomes. Simpler models might produce equal performance.

```elixir
num_of_cols = 4

model =
  Axon.input("input", shape: {nil, num_of_cols})
  |> Axon.dense(2)
  |> Axon.relu()
  |> Axon.dense(2)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(2)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(1)
  |> Axon.sigmoid()
```

### Define Loss and Optimizer

First, the deaths in the dataset.

```elixir
deaths =
  Nx.sum(train_targets)
  |> Nx.to_number()
```

Then the non-deaths.

```elixir
nondeaths = Nx.size(train_targets) - deaths
```

Determine the loss.

```elixir
loss =
  &Axon.Losses.binary_cross_entropy(
    &1,
    &2,
    negative_weight: 1 / nondeaths,
    positive_weight: 1 / deaths,
    reduction: :mean
  )
```

Then the optimizer.

```elixir
optimizer = Axon.Optimizers.adam(0.01)
```

### Model State

```elixir
model_state =
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:precision)
  |> Axon.Loop.metric(:recall)

:ok
```

### Run the Axon Loop to train the data

```elixir
model_state = model_state |> Axon.Loop.run(training_data, %{}, epochs: 2)
```

### Save the Model

```elixir
model
|> Axon.serialize(%{model_state: model_state})
|> then(&File.write!("Projects/elixir_conf/models/simple_haberman_model.axon", &1))
```

### Test the Model

<!-- livebook:{"break_markdown":true} -->

todo description

```elixir
{read_model, params} =
  File.read!("Projects/elixir_conf/models/simple_haberman_model.axon")
  |> Axon.deserialize()

model
```

todo description

```elixir
results =
  model
  |> Axon.Loop.evaluator()
  |> Axon.Loop.metric(:true_positives, "correctly_predicted_death", :running_sum)
  |> Axon.Loop.metric(:true_negatives, "correctly_predicted_nondeath_predicted", :running_sum)
  |> Axon.Loop.metric(:false_positives, "incorrectly_predicted_nondeath", :running_sum)
  |> Axon.Loop.metric(:false_negatives, "incorrectly_predicted_death", :running_sum)
  |> Axon.Loop.run(test_batch, model_state)
```

```elixir
alias VegaLite, as: Vl

r = Map.get(results, 0)

results_table = [
  %{
    "result" => "true_positives",
    "count" =>
      r
      |> Map.get("correctly_predicted_nondeath_predicted")
      |> Nx.to_number()
  },
  %{
    "result" => "true_negatives",
    "count" =>
      r
      |> Map.get("correctly_predicted_death")
      |> Nx.to_number()
  },
  %{
    "result" => "false_positives",
    "count" =>
      r
      |> Map.get("incorrectly_predicted_nondeath")
      |> Nx.to_number()
  },
  %{
    "result" => "false_negatives",
    "count" =>
      r
      |> Map.get("incorrectly_predicted_death")
      |> Nx.to_number()
  }
]

Vl.new(width: 700, height: 600)
|> Vl.data_from_values(results_table)
|> IO.inspect()
|> Vl.mark(:bar)
|> Vl.encode_field(:x, "result", type: :nominal, axis: [label_angle: 0])
|> Vl.encode_field(:y, "count", type: :quantitative)
```

```elixir
alias VegaLite, as: Vl

Vl.new(width: 700, height: 600)
|> Vl.data_from_values(DataFrame.new(df))
|> IO.inspect()
|> Vl.mark(:bar)
|> Vl.encode_field(:x, "class", type: :nominal, axis: [label_angle: 0])
|> Vl.encode_field(:y, "age", type: :quantitative)
```

```elixir
# Using a smaller batch size in this case will give the network more opportunity to learn
batch_size = 64
train_batches = Nx.to_batched_list(train_data, batch_size)
result_batches = Nx.to_batched_list(train_results, batch_size)

IO.puts("Total batches: #{Enum.count(train_batches)}")

new_params =
  new_model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adam(0.001))
  |> Axon.Loop.run(Stream.zip(train_batches, result_batches), %{}, epochs: 50, compiler: EXLA)

:ok
```

## Generate text with the new network

```elixir
generate_fn.(new_model, new_params, init_seq) |> IO.puts()
```

As you may see, It improved a lot with this new model and the extensive training. This time it knows about rules like adding a space after period.

## References

The above example is a modified version of the [Text Generation with LSTM in Axon](https://github.com/elixir-nx/axon/blob/main/notebooks/text_generator.livemd) example from the Axon library. It was heavily inspired by [this article](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) by Jason Brownlee.
