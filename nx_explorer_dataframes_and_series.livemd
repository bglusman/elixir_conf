# Nx Explorer: Dataframes and Series

## Series

```elixir
alias Explorer.DataFrame
alias Explorer.Series

s1 = Series.from_list([1, 2, 3])
IO.inspect(s1, label: "series 1")
s2 = Series.from_list(["a", "b", "c"])
IO.inspect(s2, label: "series 2")
s3 = Series.from_list([~D[2011-01-01], ~D[1965-01-21]])
IO.inspect(s3, label: "series 3")

IO.inspect(Series.dtype(s3), label: "Only the datatype of series 3")
IO.inspect(Series.size(s3), label: "Only the size of series 3")

# Printed series values max out at 50
1..100 |> Enum.to_list() |> Series.from_list() |> IO.inspect(label: "Print stops at 50")
```

```elixir
# Mixed numeric types (i.e. integers and floats) will downcast to a float:

s4 = Series.from_list([1, 2.0, 4, 5, 1.7])
IO.inspect(s4, label: "Mixed integer types downcast to float")

# Access protocol
IO.inspect(s4[1], label: "Second value of series 4")
IO.inspect(s4[-1], label: "Last value of series 4")
IO.inspect(s4[0..3], label: "First four values of series 4")
IO.inspect(s4[[0, 2, 2]], label: "First, third, and third values of series 4")

# Back to a list
IO.inspect(Series.to_list(s4), label: "Back to a list")
```

## Dataframes

```elixir
df = DataFrame.new(a: [1, 2, 3], b: ["a", "b", "c"])

IO.inspect(DataFrame.names(df), label: "Names")
IO.inspect(DataFrame.dtypes(df), label: "Datatypes")
IO.inspect(DataFrame.shape(df), label: "Shape")
IO.inspect(DataFrame.n_rows(df), label: "# of rows")
IO.inspect(DataFrame.n_columns(df), label: "# of columns")
IO.inspect(DataFrame.head(df), label: "Head")
IO.inspect(DataFrame.tail(df), label: "Tail")
```

## Dataframe Select

This Fraud Detection dataset contains transactions made by credit cards in September 2013 by European cardholders. The transactions occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

```elixir
# The Fraud Detection data set
fraud_df = Explorer.DataFrame.from_csv!("data/creditcard.csv", dtypes: [{"Time", :float}])

# The data set has these headers
# ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
# 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
# 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',
# 'Class']

IO.inspect(
  DataFrame.select(fraud_df, ["Time", "V1", "Class"]),
  label: "Values for Time, V1, and Class"
)

IO.inspect(
  DataFrame.select(fraud_df, &String.contains?(&1, "V1")),
  label: "Values that start with 'V1'"
)

IO.inspect(
  Explorer.DataFrame.select(fraud_df, &(&1 == "Class"), :keep),
  label: "Keep only the Class column"
)
```

```elixir
# Filter
# Use callbacks on the dataframe to generate boolean mask series 
# of the same size as the dataframe. Here we filter for only time values
# greater than or equal to a set value.
IO.inspect(
  DataFrame.filter(fraud_df, &Series.greater_equal(&1["Time"], 172_786.0)),
  label: "Filter for Time greater than or equal to 172786.0"
)

# You can pipe filters
IO.inspect(
  DataFrame.filter(fraud_df, &Series.greater_equal(&1["Time"], 172_786.0))
  |> DataFrame.filter(&Series.less(&1["V1"], -10.0))
  |> DataFrame.select(&(&1 == "Amount"), :keep),
  label: "Piped filters"
)
```

### Mutate

Add columns or change existing ones. You can pass in new columns as keyword arguments. It also works to transform existing columns. You may not always want to use keyword arguments. Given that column names are String.t(), it may make more sense to use a map.

```elixir
# Mutate the Class column from integer to float
DataFrame.mutate(fraud_df,
  Class: &Series.cast(&1["Class"], :float)
)
|> DataFrame.select(&(&1 == "Class"), :keep)
```

### Sampling

Random samples can give us a percent or a specific number of samples, with or without replacement, and the function is seedable.

```elixir
DataFrame.sample(fraud_df, 0.000014)
|> DataFrame.select(&(&1 in ["Time", "V12", "Class"]), :keep)
```

```elixir
DataFrame.pull(fraud_df, "Amount")
```

```elixir
DataFrame.take(fraud_df, [1, 50, 100])
|> DataFrame.select(&(&1 in ["Time", "V20", "Class"]), :keep)
```

```elixir
DataFrame.slice(fraud_df, 1500, 5)
|> DataFrame.select(&(&1 in ["Time", "V15", "Class"]), :keep)
```

```elixir
Explorer.Datasets.fossil_fuels()
|> Explorer.DataFrame.group_by("year")
|> Explorer.DataFrame.summarise(total: [:max, :min], country: [:n_unique])
```

```elixir
# Group by Class and summarize the min/max values
fraud_df
|> Explorer.DataFrame.group_by("Class")
|> Explorer.DataFrame.summarise(Time: [:max, :min])
```

## Put it all together to train a model

```elixir
defmodule DataTools do
  def to_tensor(data_frame) do
    data_frame
    |> Explorer.DataFrame.names()
    |> Enum.map(
      &(Explorer.Series.to_tensor(data_frame[&1])
        |> Nx.new_axis(-1))
    )
    |> Nx.concatenate(axis: 1)
  end

  def normalize_data(batch, target, train_max) do
    {Nx.divide(batch, train_max), target}
  end
end

# The Fraud Detection data set
fraud_df = Explorer.DataFrame.from_csv!("data/creditcard.csv", dtypes: [{"Time", :float}])

# The size of the data set
sample_size = Explorer.DataFrame.n_rows(fraud_df)

# 85% of the data set to use for training 
train_size = ceil(0.85 * sample_size)
train_df = Explorer.DataFrame.slice(fraud_df, 0, train_size)

# Select a set with the target ("Class") and without
inputs_train_df = Explorer.DataFrame.select(train_df, &(&1 == "Class"), :drop)
targets_train_df = Explorer.DataFrame.select(train_df, &(&1 == "Class"), :keep)

# convert to tensors
inputs_train = DataTools.to_tensor(inputs_train_df)
targets_train = DataTools.to_tensor(targets_train_df)

# convert tensors to lists of tensors
batched_train_inputs = Nx.to_batched(inputs_train, 2048)
batched_train_targets = Nx.to_batched(targets_train, 2048)

batched_train = Stream.zip(batched_train_inputs, batched_train_targets)

train_max = Nx.reduce_max(inputs_train, axes: [0], keep_axes: true)

normalize = fn {batch, target} ->
  {Nx.divide(batch, train_max), target}
end
```

```elixir
batched_train = batched_train |> Stream.map(&Nx.Defn.jit(normalize, [&1], compiler: EXLA))

model =
  Axon.input({nil, 30}, "input")
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(1)
  |> Axon.sigmoid()

IO.inspect(model)

fraud =
  Nx.sum(targets_train)
  |> Nx.to_number()

legit = Nx.size(targets_train) - fraud

loss =
  &Axon.Losses.binary_cross_entropy(
    &1,
    &2,
    negative_weight: 1 / legit,
    positive_weight: 1 / fraud,
    reduction: :mean
  )

optimizer = Axon.Optimizers.adam(0.01)

model_state =
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:precision)
  |> Axon.Loop.metric(:recall)
```

```elixir
model_state = model_state |> Axon.Loop.run(batched_train, %{}, epochs: 2)
```

```elixir
model
|> Axon.serialize(%{model_state: model_state})
|> then(&File.write!("simple_model.axon", &1))
```
