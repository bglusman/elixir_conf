# Nx in Action: Novice

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"},
  {:kino, "~> 0.6.2"},
  {:kino_vega_lite, "~> 0.1.1"},
  {:req, "~> 0.3.0"},
  {:vega_lite, "~> 0.1.4"}
])
```

## Question

Can we use Linear Regression to predict the CO2 emission of a car based on the size of the engine?

This question is best answered by a Linear Regression algorithm because the variable to be predicted is continuous (CO2 emissions).

The goal is to model the CO2 emissions as a function of engine size.

## Dataset

The Iris dataset was developed by Edgar Anderson in 1936. We are starting with this dataset because it is simple and easy to understand. It is also one of the datasets held within the `Explorer` library so we can interact with it easily.

It includes three iris species. There are 50 samples of each species along with additional properties. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.

##### Citation

Original Owners: R. A. Fisher (1936) The use of multiple measurements in taxonomic problems. Annals of Eugenics. 7 (2): 179â€“188. doi:10.1111/j.1469-1809.1936.tb02137.x

Iris. (1936). UCI Machine Learning Repository.

<!-- livebook:{"break_markdown":true} -->

#### Attributes

* sepal_length `float`
* sepal_width `float`
* petal_length `float`
* petal_width `float`
* species `string`

#### Observations

* There are 150 observations with 4 features each
* There are no null values
* There are 50 observations of each species (setosa, versicolor, virginica)

## Load

With the use of Explorer, we can easily load the iris data into a dataframe.

```elixir
alias Explorer.{DataFrame, Series}

df = Explorer.Datasets.iris()
```

The Dataframe `shape/1` returns a tuple of shape of the data: `{rows, columns}`:

```elixir
DataFrame.shape(df)
```

```elixir
DataFrame.n_rows(df)
```

```elixir
DataFrame.n_columns(df)
```

## Visualize

To visualize the data, use Explorer to sample a few rows of the Dataframe with `sample/3` and print it out in a table view with `table/2`

```elixir
df
|> DataFrame.sample(10)
|> DataFrame.table(limit: 3)
```

Verify there are 3 distict species of Irises in the dataset.

```elixir
df
|> DataFrame.distinct(columns: ["species"])
```

### Smart Cells

Another way to visualize the data is to use Livebook SmartCells. Set the data to the Iris dataframe and select an x-axis and y-axis.

Lets determine if our dataset is balanced. How many rows of each species are there? Evaluate the celll below to see.

<!-- livebook:{"break_markdown":true} -->

### Data Analytics

<!-- livebook:{"attrs":{"chart_title":null,"height":350,"layers":[{"chart_type":"bar","color_field":null,"color_field_aggregate":null,"color_field_type":null,"data_variable":"df","x_field":"species","x_field_aggregate":null,"x_field_type":"nominal","y_field":"__count__","y_field_aggregate":null,"y_field_type":null}],"vl_alias":"Elixir.VegaLite","width":750},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 750, height: 350)
|> VegaLite.data_from_values(df, only: ["species"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "species", type: :nominal)
|> VegaLite.encode(:y, aggregate: :count)
```

It reveals there are exactly 50 of each of the three Species features. The x-axis captures the `species` column, which is a nominal type. The y-axis is set to `Count(*)`.

##### Types

* _Quantitative_ - numerical data
* _Nominal_ - categorical data without a set order or scale
* _Ordinal_ - categorial data with a set order or scale
* _Temporal_ - data which represents a state in time

<!-- livebook:{"break_markdown":true} -->

Change the x- and y-axis to experiment with visualizing other combinations. Here is a look at `petal_width` by `species` using color to indicate the count.

<!-- livebook:{"attrs":{"chart_title":null,"height":150,"layers":[{"chart_type":"bar","color_field":"__count__","color_field_aggregate":null,"color_field_type":null,"data_variable":"df","x_field":"species","x_field_aggregate":null,"x_field_type":"nominal","y_field":"petal_width","y_field_aggregate":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":750},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 750, height: 150)
|> VegaLite.data_from_values(df, only: ["species", "petal_width"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "species", type: :nominal)
|> VegaLite.encode_field(:y, "petal_width", type: :quantitative)
|> VegaLite.encode(:color, aggregate: :count)
```

## Training Set

Earlier we used `n_rows/1` to get the number of rows in the dataset. That is useful now to determine the size needed for training and testing.

Split 85% of examples into a training set and 15% for testing.

```elixir
sample_size = train_size = ceil(0.85 * sample_size) |> IO.inspect(label: "Training")
test_size = (sample_size - train_size) |> IO.inspect(label: "Testing")

:ok
```

Use those percentages to slice the data into training and testing sets

```elixir
train_df = DataFrame.slice(df, 0, train_size)
test_df = DataFrame.slice(df, 0, test_size)

:ok
```

## Split Features from Targets

Split both the training and testing sets into two sets: one set of features and other set of targets because Axon expects tuples of {features, targets} to train a model. The target in this class is the "class" column, the 1 if the patient survived 5 years or longer or 0 if they did not. The rest of the columns are features.

```elixir
train_df_inputs =
  DataFrame.select(train_df, &(&1 == "class"), :drop) |> IO.inspect(label: "features")

train_df_targets =
  DataFrame.select(train_df, &(&1 == "class"), :keep) |> IO.inspect(label: "targets")

test_df_inputs = DataFrame.select(test_df, &(&1 == "class"), :drop)
test_df_targets = DataFrame.select(test_df, &(&1 == "class"), :keep)

:ok
```

### Convert the sets to tensors

<!-- livebook:{"break_markdown":true} -->

The training and testing tests are still Explorer DataFrames, but Axon expects Nx Tensors. Convert the sets into Nx tensors.

```elixir
to_tensor = fn data_frame ->
  data_frame
  |> DataFrame.names()
  |> Enum.map(
    &(Series.to_tensor(data_frame[&1])
      |> Nx.new_axis(-1))
  )
  |> Nx.concatenate(axis: 1)
end

train_inputs = to_tensor.(train_df_inputs) |> IO.inspect(label: "features")
train_targets = to_tensor.(train_df_targets) |> IO.inspect(label: "targets")

test_inputs = to_tensor.(test_df_inputs)
test_targets = to_tensor.(test_df_targets)

:ok
```

### Convert to lists of tensors

<!-- livebook:{"break_markdown":true} -->

Axon expects minibatches for training. Nx `to_bached/3` converts a tensor to a stream of tensor batches. From the [docs](https://hexdocs.pm/nx/Nx.html#to_batched_list/3):

> The first dimension (axis 0) is divided by `batch_size`. In case the dimension cannot be evenly divided by `batch_size`, you may specify what to do with leftover data using `:leftover`. `:leftover` must be one of `:repeat` or `:discard`. `:repeat` repeats the first `n` values to make the last batch match the desired batch size. `:discard` discards excess elements.

The training set has 261 rows and the testing set has 45. We will set the `batch_size` to 15 and the `leftover` to `:repeat` since our dataset is small.

```elixir
# Training set size: 261; testing set size: 45
batch_size = 20

train_batch_inputs = Nx.to_batched_list(train_inputs, batch_size, leftover: :repeat)

train_batch_targets =
  Nx.to_batched_list(train_targets, batch_size, leftover: :repeat)
  |> IO.inspect(label: "batched targets")

test_batch_inputs = Nx.to_batched_list(test_inputs, batch_size, leftover: :repeat)
test_batch_targets = Nx.to_batched_list(test_targets, batch_size, leftover: :repeat)

:ok
```

Zip the batched features with the batched targets for each set, training and testing.

```elixir
train_batch = Stream.zip(train_batch_inputs, train_batch_targets)
test_batch = Stream.zip(test_batch_inputs, test_batch_targets)

:ok
```

### Normalize

<!-- livebook:{"break_markdown":true} -->

Normalize the input data. Ideally, each column should be a value between 1 and 0. This increases the performance of the model. The simpliest way to achieve normalization is to divide each feature by the maximum value of the feature.

For this dataset we need to normalize the 3 feature columns. The target column is already a 1 or 0.

```elixir
train_max =
  Nx.reduce_max(train_inputs, axes: [0], keep_axes: true)
  |> IO.inspect(label: "max value of each feature")

normalize = fn {batch, target} ->
  normalized_feature = Nx.divide(batch, train_max)
  {normalized_feature, target}
end

training_data =
  train_batch
  |> Stream.map(&Nx.Defn.jit(normalize, [&1]))

:ok
```

## Model

### Define the Model

The model trained here is a feed-forward neural network. You can choose among many established models, but this one is good to classify our features as predictive of surgery survival or not.

The dataset is relatively small, with a very small set of input features. This means the model can do well with a minimum number of intermediate layers. Another way to think of this is the number of neuronal layers and the number of neurons comprising each layer.

For most problems, good performance can be achieved by determining the hidden layer using  two rules:

1. the number of hidden layers equals one
2. the number of neurons in that layer is the mean of the neurons in the input and output layers

The size of the input layer is equal to the number of features (columns) in your data. Some NN configurations add one additional node for a bias term. In our case, the number is *4*.The size of the output layer is typically 1.

* 3 hidden layers with a hidden size of 2

TODO define regression vs classification

This is a starting point. Changing the hidden size or activations will produce different outcomes. Simpler models might produce equal performance.

```elixir
num_of_cols = 3

# Axon.input("input", shape: {nil, num_of_cols})
model =
  Axon.input({nil, num_of_cols}, "input")
  |> Axon.dense(2)
  |> Axon.relu()
  |> Axon.dense(2)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(2)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(1)
  |> Axon.sigmoid()
```

### Define Loss and Optimizer

First, the deaths in the dataset.

```elixir
deaths =
  Nx.sum(train_targets)
  |> Nx.to_number()
```

Then the non-deaths.

```elixir
nondeaths = Nx.size(train_targets) - deaths
```

#### Loss Funciton

The loss function measures how well a model fit the data set.The bigger the difference between the prediction and the ground truth (target or class), the higher the loss function value.

```elixir
loss =
  &Axon.Losses.binary_cross_entropy(
    &1,
    &2,
    negative_weight: 1 / nondeaths,
    positive_weight: 1 / deaths,
    reduction: :mean
  )
```

Then the optimizer.

```elixir
optimizer = Axon.Optimizers.adam(0.01)
```

### Model

#### Train and run the model

When training, set the loss function and optimizer. When running, set the number of epochs.

##### Loss Function

The loss function measures how well a model fit the data set.The bigger the difference between the prediction and the ground truth (target or class), the higher the loss function value.

##### Optimizer

Implementations of common gradient-based optimization algorithms. Customize the Optimizer by passing in a learning rate. The learning rate is the amount the weights are updated during training. It is a hyperparameter. Use hyperparameters to fine tune your model.

##### Metric

Metrics are used to measure the performance and compare performance of models.

* Accuracy tells you how many times the model was correct overall
* Precision is how good the model is at predicting a specific category
* Recall tells you how many times the model was able to detect a specific category

##### Epochs

An epoch refers to one complete pass of the training data through the algorithm. It is  a hyperparameter. The maximum epochs to run the Axon loop for. Must be non-negative integer (default is 1).

```elixir
model_state =
  model
  |> Axon.Loop.trainer(:mean_squared_error, Axon.Optimizers.adamw(0.01))
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(training_data, %{}, epochs: 350)
```

### Save the Model

```elixir
model
|> Axon.serialize(%{model_state: model_state})
|> then(&File.write!("Projects/elixir_conf/models/simple_haberman_model.axon", &1))
```

### Testing

<!-- livebook:{"break_markdown":true} -->

todo description of the Axon evaluator, metrics, and run

<!-- livebook:{"break_markdown":true} -->

|                          | Positive (Actual) | Negative (Actual) |
| ------------------------ | ----------------- | ----------------- |
| **Positive (Predicted)** | True Positive     | False Positive    |
| **Negative (Predicted)** | True Negative     | False Negative    |

***

|                              | Truth: Yes hot dog! | Truth: No hot dog   |
| ---------------------------- | ------------------- | ------------------- |
| **Prediction: Yes hot dog!** | Hot Dog (food)      | Hot Dog (Dachshund) |
| **Prediction: No hot dog**   | Chili Dog (food)    | Pizza (food)        |

***

##### Haberman Dataset

|                                  | Truth: Survived | Truth: Did not surivive |
| -------------------------------- | --------------- | ----------------------- |
| **Prediction: Survived**         | true_positive   | false_positive          |
| **Prediction: Did not surivive** | false_negative  | true_negative           |

```elixir
{read_model, _params} =
  File.read!("Projects/elixir_conf/models/simple_haberman_model.axon")
  |> Axon.deserialize()

results =
  read_model
  |> Axon.Loop.evaluator()
  |> Axon.Loop.metric(:true_positives, "true_positive", :running_sum)
  |> Axon.Loop.metric(:true_negatives, "true_negative", :running_sum)
  |> Axon.Loop.metric(:false_positives, "false_positive", :running_sum)
  |> Axon.Loop.metric(:false_negatives, "false_negative", :running_sum)
  |> Axon.Loop.run(test_batch, model_state)
  |> Map.get(0)
  |> Enum.map(fn {k, v} ->
    %{
      "result" => k |> String.replace("_", " ") |> String.capitalize(),
      "count" => Nx.to_number(v)
    }
  end)

alias VegaLite, as: Vl

Vl.new(width: 500, height: 500)
|> Vl.data_from_values(results)
|> Vl.mark(:bar)
|> Vl.encode_field(:x, "result", type: :nominal, axis: [label_angle: 0])
|> Vl.encode_field(:y, "count", type: :quantitative)
```

## Predict

### TODO define prediction

```elixir
[prediction_batch | _] = train_batch_inputs

IO.inspect(prediction_batch)

{read_model, params} =
  File.read!("Projects/elixir_conf/models/simple_haberman_model.axon")
  |> Axon.deserialize()

# prediction_batch = Nx.to_batched_list(prediction_batch, 2)
Axon.predict(read_model, params, prediction_batch)
```

## Advanced Challenge

Can you increase the accuracy of your predictions using Multiple Regression by adding more variables, like the weight of the car?

The goal would be to model the CO2 emissions as a function of several car engines features.

You could even get a larger dataset with additional features [here](https://raw.githubusercontent.com/amercader/car-fuel-and-emissions/master/data.csv).
