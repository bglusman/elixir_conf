# Nx in Action / Novice / Train

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"}
])
```

## Question

Can we predict passangers who will survive the Titanic disaster?

<!-- livebook:{"break_markdown":true} -->

![](images/titanic.png)

## Load

With the use of Explorer, load the normalized training set you saved in the last section.

```elixir
alias Explorer.{DataFrame, Series}

csv = "elixir_conf/data/titanic/normalized/train.csv"
df = DataFrame.from_csv!(csv)
```

## Train

Define the necessary values.

```elixir
targets = DataFrame.select(df, &(&1 == "Survived"), :keep)
features = DataFrame.select(df, &(&1 == "Survived"), :drop)

batch_size = 200
```

```elixir
to_tensor = fn df ->
  df
  |> Explorer.DataFrame.names()
  |> Enum.map(&(Explorer.Series.to_tensor(df[&1]) |> Nx.new_axis(-1)))
  |> Nx.concatenate(axis: 1)
end

targets = to_tensor.(targets)
features = to_tensor.(features)

batched_targets = Nx.to_batched_list(targets, batch_size)
batched_features = Nx.to_batched_list(features, batch_size)
train_set = Stream.zip(batched_features, batched_targets)
{_rows, cols} = DataFrame.shape(df)
train_cols = cols - 1
```

```elixir
epochs = 50
loss = :mean_absolute_error
learning_rate = 0.001
dropout_rate = 0.1
optimizer = Axon.Optimizers.adamw(learning_rate)

model =
  Axon.input({batch_size, train_cols}, "inputs")
  |> Axon.dense(10)
  |> Axon.dropout(rate: dropout_rate)
  |> Axon.dense(1)
```

```elixir
model =
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

```elixir
model =
  Axon.input({batch_size, train_cols}, "inputs")
  |> Axon.dense(25)
  |> Axon.dropout(rate: dropout_rate)
  |> Axon.dense(5)
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

Explain binary cross entropy

```elixir
model =
  Axon.input({batch_size, train_cols}, "input")
  |> Axon.dense(25)
  |> Axon.relu()
  |> Axon.dense(25)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(25)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(1)
  |> Axon.sigmoid()
```

```elixir
survived = Nx.sum(targets) |> Nx.to_number()
notsurvived = Nx.size(targets) - survived

loss =
  &Axon.Losses.binary_cross_entropy(
    &1,
    &2,
    negative_weight: 1 / survived,
    positive_weight: 1 / notsurvived,
    reduction: :mean
  )
```

```elixir
optimizer = Axon.Optimizers.adam(1.0e-3)

model_state =
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:precision)
  |> Axon.Loop.metric(:recall)
  |> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

```elixir
defmodule LinReg do
  import Nx.Defn

  @epochs 100
  @learning_rate 0.02

  defn predict({m, b}, x) do
    m * x + b
  end

  defn loss(params, x, y) do
    y_pred = predict(params, x)
    Nx.mean(Nx.power(y - y_pred, 2))
  end

  defn update({m, b} = params, inp, tar) do
    {grad_m, grad_b} = grad(params, &loss(&1, inp, tar))

    {
      m - grad_m * @learning_rate,
      b - grad_b * @learning_rate
    }
  end

  def train(data) do
    # match type & shape of inputs {4, 1} & targets {1}
    init_feature = Nx.random_normal({200, 6}, 0.0, 0.1)
    init_target = Nx.tensor([1])
    init_params = {init_feature, init_target}

    for _ <- 1..@epochs, reduce: init_params do
      acc ->
        data
        |> Enum.reduce(
          acc,
          fn {x, y}, current_params ->
            IO.inspect(update(current_params, x, y), label: "gradient")
          end
        )
    end
  end
end
```

```elixir
model = LinReg.train(train_set)

test_set
|> Enum.each(fn {input, truth} ->
  prediction =
    LinReg.predict(model, input)
    |> scalar.()

  IO.inspect(
    "Actual: #{scalar.(truth)}. Predicted: #{prediction}. Loss: #{scalar.(truth) - prediction}"
  )
end)
```

## Advanced Challenge
