# Nx in Action / Novice / Train

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"}
])
```

## Question

Can we predict passangers who will survive the Titanic disaster?

<!-- livebook:{"break_markdown":true} -->

![](images/titanic.png)

## Metrics

#### Accuracy

_How accurate is the model at making predictions on unseen data?_

The number of correct predictions relative to the total number of predictions

`(true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)`

#### Precision

_Out of all positive predictions made by the model, what percentage are truly positive?_

The number of actual positive classes (`true_pos`) found in the dataset relative to the number of actual positive classes (`true_pos`) plus classes that were falsely identified as positive (`false_pos`).

(true_pos) / (true_pos + false_pos)

#### Recall

_Of all of the actual positive classes in the dataset, how many of them did the model recall?_

The number of actual positive classes (`true_pos`) relative to the number of actual positive classes (`true_pos`) plus classes that were falsely identified as negative (`false_neg`) — those misidentified as negative.

`(true_pos) / (true_pos + false_neg)`

## Load

With the use of Explorer, load the normalized training set you saved in the last section.

```elixir
alias Explorer.DataFrame
alias Explorer.Series

csv = "elixir_conf/data/titanic/normalized/train.csv"
df = DataFrame.from_csv!(csv)
```

## Train

Define the necessary values.

```elixir
targets = DataFrame.select(df, &(&1 == "Survived"), :keep)
features = DataFrame.select(df, &(&1 == "Survived"), :drop)

batch_size = 100
epochs = 25
loss_rate = 0.001
```

```elixir
to_tensor = fn df ->
  df
  |> Explorer.DataFrame.names()
  |> Enum.map(&(Explorer.Series.to_tensor(df[&1]) |> Nx.new_axis(-1)))
  |> Nx.concatenate(axis: 1)
end

targets = to_tensor.(targets)
features = to_tensor.(features)

batched_targets = Nx.to_batched_list(targets, batch_size)
batched_features = Nx.to_batched_list(features, batch_size)
train_set = Stream.zip(batched_features, batched_targets)
{_rows, cols} = DataFrame.shape(df)
train_cols = cols - 1
```

!!!!!!!!!!!!!!!!!!!!!!!!!!

Our problem is a binary classification problem. That means we want to classify passengers in one of two classes: survived or not survived. That means we want our neural network to predict a probability between 0 and 1. 

Probabilities closer to 1 indicate a higher confidence that the passenger survived. Probabilities closer to 0 represent a lower confidence that an example passenger survived. 

`sigmoid` is an activation function which always returns a value between 0 and 1. Making it a good choice to return the probability we’re looking for.

!!!!!!!!!!!!!!!!!!!!!!!!!!!

Like this:
(shape will be wrong)

```elixir
model =
  Axon.input({nil, 4, 32, 32})
  |> Axon.conv(32, kernel_size: {3, 3})
  |> Axon.batch_norm()
  |> Axon.relu()
  |> Axon.max_pool(kernel_size: {2, 2})
  |> Axon.conv(64, strides: [2, 2])
  |> Axon.batch_norm()
  |> Axon.relu()
  |> Axon.max_pool(kernel_size: {2, 2})
  |> Axon.conv(32, kernel_size: {3, 3})
  |> Axon.batch_norm()
  |> Axon.relu()
  |> Axon.global_avg_pool()
  |> Axon.dense(1, activation: :sigmoid)
```
and then run it:

```elixir
model
  |> Axon.Loop.trainer(:binary_cross_entropy, :adam, log: 1)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.validate(model, val_data)
  |> Axon.Loop.run(train_set, epochs: 5)

model
|> Axon.Loop.evaluator(model_state)
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(test_data)
```

TODO explain categorical cross entropy

```elixir
model =
  Axon.input({batch_size, train_cols}, "input")
  |> Axon.flatten()
  |> Axon.dense(128, activation: :relu)
  |> Axon.dense(10, activation: :softmax)
```

```elixir
trained_model =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, :adam)
  |> Axon.Loop.metric(:accuracy, "Accuracy")
  # |> Axon.Loop.metric(:precision)
  # |> Axon.Loop.metric(:recall)
  |> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

```elixir
model =
  Axon.input({batch_size, train_cols}, "input")
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.1)
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.1)
  |> Axon.dense(1)
  |> Axon.dense(3, activation: :softmax)
```

Explain binary cross entropy

```elixir
model =
  Axon.input({batch_size, train_cols}, "input")
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(1)
  |> Axon.sigmoid()
```

```elixir
survived = Nx.sum(targets) |> Nx.to_number()
notsurvived = Nx.size(targets) - survived

loss =
  &Axon.Losses.binary_cross_entropy(
    &1,
    &2,
    negative_weight: 1 / survived,
    positive_weight: 1 / notsurvived,
    reduction: :mean
  )
```

```elixir
optimizer = Axon.Optimizers.adam(1.0e-3)

model_state =
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:precision)
  |> Axon.Loop.metric(:recall)
  |> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

```elixir
defmodule LinReg do
  import Nx.Defn

  @epochs 100
  @learning_rate 0.02

  defn predict({m, b}, x) do
    m * x + b
  end

  defn loss(params, x, y) do
    y_pred = predict(params, x)
    Nx.mean(Nx.power(y - y_pred, 2))
  end

  defn update({m, b} = params, inp, tar) do
    {grad_m, grad_b} = grad(params, &loss(&1, inp, tar))

    {
      m - grad_m * @learning_rate,
      b - grad_b * @learning_rate
    }
  end

  def train(data) do
    # match type & shape of inputs {4, 1} & targets {1}
    init_feature = Nx.random_normal({200, 4}, 0.0, 0.1)
    init_target = Nx.tensor([1], type: {:s, 64})
    init_params = {init_feature, init_target}

    for _ <- 1..@epochs, reduce: init_params do
      acc ->
        data
        |> Enum.reduce(
          acc,
          fn {x, y}, current_params ->
            IO.inspect(update(current_params, x, y), label: "gradient")
          end
        )
    end
  end
end
```

```elixir
model = LinReg.train(train_set)
```

```elixir
model =
  Axon.input({nil, 1, 891, 7}, "input")
  |> Axon.conv(16, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.conv(32, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.dropout(rate: 0.1)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.conv(32, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.dropout(rate: 0.1)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.conv(32, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.dropout(rate: 0.2)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.flatten()
  |> Axon.dense(16, activation: :relu)
  |> Axon.dropout(rate: 0.1)
  |> Axon.dense(8, activation: :relu)
  |> Axon.dense(2, activation: :softmax)
```

```elixir
epochs = 30

model =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adamw(0.000005))
  # |> Axon.Loop.metric(:accuracy, "Accuracy")
  |> Axon.Loop.run(inputs, %{}, epochs: epochs)
```

## Advanced Challenge
