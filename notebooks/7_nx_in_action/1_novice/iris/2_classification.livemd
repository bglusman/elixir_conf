# Nx in Action/ Novice/ Iris / Classification

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"},
  {:kino, "~> 0.6.2"},
  {:kino_vega_lite, "~> 0.1.1"},
  {:req, "~> 0.3.0"},
  {:scidata, "~> 0.1.9"},
  {:vega_lite, "~> 0.1.4"}
])
```

## Question

Can we predict Iris species using a classification model?

<!-- livebook:{"break_markdown":true} -->

### Classification Algorithm

We will train a multiclass classification model on this dataset.

It will map the input values (`x`) to a discrete output variable (`y`) and makes predictions by categorizing data into classes based on independent variables.

#### Outcome is CATEGORICAL: predicts a discrete (finite) class label.

##### Multiclass classification (more than two outcome labels)

* Iris species prediction  
  * setosa / versicolor / virginica

## Load

Load the data from Scidata.

```elixir
{features, targets} = Scidata.Iris.download()
```

Normalize the data as we did in the last section.

```elixir
feature_max =
  Nx.tensor(features)
  |> Nx.reduce_max(axes: [0], keep_axes: true)

inputs =
  Enum.map(features, fn feature ->
    Nx.divide(Nx.tensor(feature), feature_max)
  end)

targets =
  Enum.map(targets, fn t ->
    Nx.tensor([t])
  end)

df = Enum.zip([inputs, targets])
```

## Train

### Define the Model

1. Epochs - how many training iterations to take over the dataset
2. Learning rate - size of the optimization step at each iteration
3. Loss - measure of the distance between the prediction from the label
4. Metric (optional) - choose metric to measure mdoel's performance

<!-- livebook:{"break_markdown":true} -->

### Epochs

An epoch refers to one complete pass of the training data through the algorithm. It is  a hyperparameter. The maximum epochs to run the Axon loop for. Must be non-negative integer (default is 1).

<!-- livebook:{"break_markdown":true} -->

### Learning Rate

Implementations of common gradient-based optimization algorithms. Customize the Optimizer by passing in a learning rate. The learning rate is the amount the weights are updated during training. It is a hyperparameter. Use hyperparameters to fine tune your model.

<!-- livebook:{"break_markdown":true} -->

### Loss Function

The loss function measures how well a model fit the data set.The bigger the difference between the prediction and the ground truth (target or class), the higher the loss function value.

```elixir
epochs = 25
learning_rate = 0.001
optimizer = Axon.Optimizers.adamw(learning_rate)
loss = :mean_absolute_error

dropout_rate = 0.1
input_shape = {1, 4}
```

Each layer helps the neural network learn accurate representations of the input data.

```elixir
model =
  Axon.input(input_shape, "inputs")
  |> Axon.dense(10)
  |> Axon.dropout(rate: dropout_rate)
  |> Axon.dense(1)
```

```elixir
model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.run(df, %{}, epochs: epochs)
```

### Metric

Metrics are used to measure the performance of models.

##### Accuracy

The number of times the model was correct overall

_How accurate is the model at making predictions on unseen data?_

`(true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)`

##### Precision

Tells you how good the model is at prediction a specific category.

_Out of all positive predictions made by the model, what percentage are truly positive?_

The number of actual positive classes (`true_pos`) found in the dataset relative to the number of actual positive classes (`true_pos`) plus classes that were falsely identified as positive (`false_pos`).

(true_pos) / (true_pos + false_pos)

##### Recall

Tells you how many times the model was able to detect a specific category.

_Of all of the actual positive classes in the dataset, how many of them did the model recall?_

The number of actual positive classes (`true_pos`) relative to the number of actual positive classes (`true_pos`) plus classes that were falsely identified as negative (`false_neg`) â€” those misidentified as negative.

`(true_pos) / (true_pos + false_neg)`

###### Confusion Matrix

|                          | Positive (Actual) | Negative (Actual) |
| ------------------------ | ----------------- | ----------------- |
| **Positive (Predicted)** | True Positive     | False Positive    |
| **Negative (Predicted)** | True Negative     | False Negative    |

<!-- livebook:{"break_markdown":true} -->

#### Recall Metric

```elixir
model =
  Axon.input(input_shape, "inputs")
  |> Axon.dense(25)
  |> Axon.dropout(rate: dropout_rate)
  |> Axon.dense(5)
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:recall)
  |> Axon.Loop.run(df, %{}, epochs: epochs)
```

#### Accuracy Metric

```elixir
# epochs = 15

model =
  Axon.input(input_shape, "inputs")
  |> Axon.dense(75)
  |> Axon.dropout(rate: dropout_rate)
  |> Axon.dense(50)
  |> Axon.dropout(rate: dropout_rate)
  |> Axon.dense(5)
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(df, %{}, epochs: epochs)
```

```elixir
loss = :categorical_cross_entropy

model =
  Axon.input(input_shape, "inputs")
  |> Axon.dense(128, activation: :relu)
  |> Axon.layer_norm()
  |> Axon.dropout()
  |> Axon.dense(10, activation: :softmax)
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.run(df, %{}, epochs: epochs)
```

```elixir
loss = :categorical_cross_entropy

model =
  Axon.input(input_shape, "inputs")
  |> Axon.dense(128, activation: :relu)
  |> Axon.layer_norm()
  |> Axon.dropout(rate: dropout_rate)
  |> Axon.dense(5, activation: :softmax)
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(df, %{}, epochs: epochs)
```

HELP! TODO: need help understanding why adding softmax as the final activation function makes the accuracy so much worse?

Our problem is a multi-class classification problem. That means we want to classify irises into one of 3 classes: setosa, versicolor, or virginica.

We'll use `:softmax` as our activation function. It returns the probability of each class. It is most commonly used as an activation function for the last layer of the neural network in the case of multi-class classification.
