# Nx in Action/ Novice/ Iris / Data

```elixir
Mix.install([
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"},
  {:kino, "~> 0.6.2"},
  {:kino_vega_lite, "~> 0.1.1"},
  {:scidata, "~> 0.1.9"},
  {:vega_lite, "~> 0.1.4"}
])
```

## The Plan

1. Define the problem
2. Pepare the data
3. Evaluate the algorithms
4. Choose the model
5. Enhance the model
6. Make the predictions

## Define the problem

Can we predict Iris species?

## Prepare the data

The Iris dataset was developed by Edgar Anderson in 1936. We are starting with this dataset because it is simple and easy to understand. It is also one of the datasets held within the Nx ecosystem (`Explorer` and `Scidata`) so we can interact with it easily.

It includes three iris species. There are 50 samples of each species along with additional properties. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.

<span style="font-size: 14px;">
Citation: R. A. Fisher (1936) <span style="font-style: italic;">The use of multiple measurements in taxonomic problems. Annals of Eugenics.
</span>

![](images/iris.png)

<!-- livebook:{"break_markdown":true} -->

### Features

* sepal_length `float`
* sepal_width `float`
* petal_length `float`
* petal_width `float`
* species `string`

### Observations

* There are 150 observations with 4 features each
* There are no null values
* There are 50 observations of each species (setosa, versicolor, virginica)

## Load: Scidata

<img src="https://static.thenounproject.com/png/1111032-200.png" alt="Brain icon" style="width: 55px; float: left; margin-right: 40px;" />

The Nx ecosystem library, Scidata, provides the Iris dataset for us. It is returned as a tuple of features (inputs) and targets (classes).

```elixir
{features, targets} = Scidata.Iris.download()
```

```elixir
Enum.count(features) |> IO.inspect(label: "features")
Enum.count(features) |> IO.inspect(label: "targets")
:ok
```

```elixir
feature_max =
  Nx.tensor(features)
  |> Nx.reduce_max(axes: [0], keep_axes: true)
```

```elixir
inputs =
  Enum.map(features, fn feature ->
    Nx.divide(Nx.tensor(feature), feature_max)
  end)
```

```elixir
targets =
  Enum.map(targets, fn t ->
    Nx.tensor([t])
  end)
```

Look closely at the targets above. The species looks to be grouped in order 0-2 within the dataset. If we were to split this dataset into a training set and a testing set without shuffling first, we would get two unbalanced sets.

```elixir
df = Enum.zip([inputs, targets]) |> Enum.shuffle()
```

## Load: Explorer

<img src="https://raw.githubusercontent.com/elixir-nx/explorer/main/explorer.png" alt="Brain icon" style="width: 200px; float: left; margin-right: 20px;" />

The Iris dataset is also easily available from the Nx ecosystem library, Explorer. Explorer provides the Iris dataset differently, but such that it can be readily loaded into a dataframe.

```elixir
alias Explorer.DataFrame
alias Explorer.Series

expdf = Explorer.Datasets.iris()

DataFrame.shape(expdf)
```

```elixir
DataFrame.n_rows(expdf)
```

```elixir
DataFrame.n_columns(expdf)
```

## Visualize

<img src="https://static.thenounproject.com/png/5094962-200.png" style="width: 75px; float: left; margin-right: 20px;" />

To visualize the data, use Explorer to sample a few rows of the Dataframe with `sample/3` and print it out in a table view with `table/2`

```elixir
expdf
|> DataFrame.sample(10)
|> DataFrame.table(limit: 3)
```

Verify there are 3 distict species of Irises in the dataset.

```elixir
expdf
|> DataFrame.distinct(columns: ["species"])
```

### Smart Cells

Another way to visualize the data is to use Livebook SmartCells. Set the data to the Iris dataframe and select an x-axis and y-axis.

Lets determine if our dataset is balanced. How many rows of each species are there?

The cell below uses data from the `expdf` variable we set using the Explorer Iris dataset. It is a "bar" chart with "species" type "nominal" as the x-axis and "COUNT(*)" as the y-axis.

Evaluate the celll below.

<!-- livebook:{"break_markdown":true} -->

### Data Analytics

<!-- livebook:{"attrs":{"chart_title":null,"height":350,"layers":[{"chart_type":"bar","color_field":null,"color_field_aggregate":null,"color_field_type":null,"data_variable":"expdf","x_field":"species","x_field_aggregate":null,"x_field_type":"nominal","y_field":"__count__","y_field_aggregate":null,"y_field_type":null}],"vl_alias":"Elixir.VegaLite","width":750},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 750, height: 350)
|> VegaLite.data_from_values(expdf, only: ["species"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "species", type: :nominal)
|> VegaLite.encode(:y, aggregate: :count)
```

There are exactly 50 of each of the three Species features; we have a balanced dataset.

<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #800080;height: 5.0px;" />

<img src="https://static.thenounproject.com/png/2212696-200.png" alt="Brain icon" style="width: 85px; float: right; margin-right: 40px;" />

##### Types

* _Quantitative_ - numerical data
* _Nominal_ - categorical data without a set order or scale
* _Ordinal_ - categorial data with a set order or scale
* _Temporal_ - data which represents a state in time

<hr style="background-color: #800080;height: 5.0px;" />

<!-- livebook:{"break_markdown":true} -->

Change the x- and y-axis to experiment with visualizing other combinations. Here is a look at `petal_width` by `species` using color to indicate the count.

<!-- livebook:{"attrs":{"chart_title":null,"height":150,"layers":[{"chart_type":"point","color_field":null,"color_field_aggregate":null,"color_field_type":null,"data_variable":"expdf","x_field":"sepal_length","x_field_aggregate":null,"x_field_type":"quantitative","y_field":"sepal_width","y_field_aggregate":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":750},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 750, height: 150)
|> VegaLite.data_from_values(expdf, only: ["sepal_length", "sepal_width"])
|> VegaLite.mark(:point)
|> VegaLite.encode_field(:x, "sepal_length", type: :quantitative)
|> VegaLite.encode_field(:y, "sepal_width", type: :quantitative)
```

## Create a Training Set

Earlier we used `n_rows/1` to get the number of rows in a dataset. That is useful now to determine the size needed for training and testing.

The training set should have the majority of the data. For example, you could decide to put 85% of the rows in a training set and 15% in a testing set. The training data “trains” the machine learning model. The test set will be used to test the accuracy of the model's predictions on unseen examples.

Split 85% of examples into a training set: `slice(df, offset, length)`

We will replicate these steps in the next section to create a test set with 15% of the rows.

```elixir
sample_size = DataFrame.n_rows(expdf)
train_size = ceil(0.85 * sample_size)
```

```elixir
train_df = DataFrame.slice(expdf, 0, train_size)
```

<hr style="background-color: #EED202; height: 5px;" />

<img src="https://static.thenounproject.com/png/1038260-200.png" alt="Caution icon" style="width: 50px; display: block; margin-left: auto; margin-right: auto;" />

<span style="font-size: 16px; font-style: italic;">
This is a novice exercise meant as a warm up to putting the Nx ecosystem into action. You'll note the Iris dataset is very small.This method we are using is not suggestd for real world problems since splitting the dataset into a training and testing set reduces the size even more.

Since the goal is to predict iris species using a dataset the model has not seen before, ideally we would have a much larger training and testing set. Remember back in an earlier lesson when we discussed the challenges of machine learning. Here we are at risk of **overfitting** the training data.
</span>

<hr style="background-color: #EED202;height: 5px;" />

## Features & Targets

The Explorer dataset is not split into features and targets like the Scidata set is.

The target is the `species` column. The rest of the columns are features. Use Dataframe `select/3` to drop the `species` column to create a **feature set** and to keep only the `species` column for a **target set**.

```elixir
feature_set =
  train_df
  |> DataFrame.select(&(&1 == "species"), :drop)
```

```elixir
target_set =
  train_df
  |> DataFrame.select(&(&1 == "species"), :keep)
```

![](images/binary.png)

<!-- livebook:{"break_markdown":true} -->

### Feature Normalization

<!-- livebook:{"break_markdown":true} -->

The features are `floats` such as 5.6. The values need to be normalized between 0 and 1.

First, get the features from the `feature_set`:

```elixir
features = DataFrame.names(feature_set)
```

Enumerate over that lis and inspect the name on each iteration.

```elixir
features
|> Enum.map(fn name ->
  IO.inspect(name)
end)
```

Do the same enumeration again, only this time look at the feature's values.

```elixir
features
|> Enum.map(fn name ->
  IO.inspect(feature_set[name])
end)
```

They are each a Series. That makes it easy to convert them to tensors so we can find the maximum value of the feature. Like this:

```elixir
features
|> Enum.map(fn name ->
  s = Series.to_tensor(feature_set[name])
  Nx.reduce_max(s, axes: [0], keep_axes: true)
end)
```

Now we use the maximum value of the feature to normalize each of the values.

Do do that, we want to transform the orignal Series into the value of the original value divided by the maximum value.

Nx division results in a tensor. We need a `float`. Convert the tensor to a flat list and take the only element.

```elixir
features
|> Enum.map(fn name ->
  s = Series.to_tensor(feature_set[name])
  max = Nx.reduce_max(s, axes: [0], keep_axes: true)

  Series.transform(feature_set[name], fn v ->
    Nx.divide(v, max)
    |> Nx.to_flat_list()
    |> List.first()
  end)
end)
```

Transform that into a tensor.

```elixir
features
|> Enum.map(fn name ->
  s = Series.to_tensor(feature_set[name])
  max = Nx.reduce_max(s, axes: [0], keep_axes: true)

  Series.transform(feature_set[name], fn v ->
    Nx.divide(v, max)
    |> Nx.to_flat_list()
    |> List.first()
  end)
  |> Series.to_tensor()
end)
```

Pipe the tensor into `Nx.new_axis/3` and set the axis to negative, which will start from the back.

End the enumeration and concatenate all the tensors on a single axis.

```elixir
inputs =
  features
  |> Enum.map(fn name ->
    s = Series.to_tensor(feature_set[name])
    max = Nx.reduce_max(s, axes: [0], keep_axes: true)

    Series.transform(feature_set[name], fn v ->
      Nx.divide(v, max)
      |> IO.inspect(label: "n")
      |> Nx.to_flat_list()
      |> List.first()
    end)
    |> Series.to_tensor()
    |> Nx.new_axis(-1)
  end)
  |> Nx.concatenate(axis: 1)
```

## Target Normalization

Our targets are currently strings, i.e. "Iris-setosa". Normalize them by converting them to integers.

##### Label

* 0: Iris Setosa
* 1: Iris Versicolour
* 2: Iris Virginica

```elixir
target_set
```

Access only that column, then transform the value such that each type gets its integer value.

```elixir
targets =
  target_set["species"]
  |> Series.transform(fn x ->
    case String.downcase(x) do
      "iris-setosa" -> 0
      "iris-versicolour" -> 1
      "iris-versicolor" -> 1
      "iris-virginica" -> 2
    end
  end)
```

Then convert the Series to a tensor.

```elixir
input_targets =
  targets
  |> Series.to_tensor()
  |> Nx.new_axis(-1)
```

## TODO: add link to next section
