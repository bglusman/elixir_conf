# Nx in Action/ Novice/ Iris / Train Neural Network Model

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"},
  {:kino, "~> 0.6.2"},
  {:kino_vega_lite, "~> 0.1.1"},
  {:req, "~> 0.3.0"},
  {:scidata, "~> 0.1.9"},
  {:vega_lite, "~> 0.1.4"}
])
```

## The Plan

1. Define the problem
2. Pepare the data
3. Evaluate the algorithms
4. Choose the model
5. Enhance the model
6. Make the predictions

## Define the problem

Can we predict Iris species?

## Prepare the data

```elixir
{inputs, targets} = Scidata.Iris.download()
count = Enum.count(inputs)
```

```elixir
imax =
  Nx.tensor(inputs)
  |> Nx.reduce_max(axes: [0], keep_axes: true)

tmax = 2

inputs =
  Enum.map(inputs, fn i ->
    Nx.divide(Nx.tensor(i), imax)
  end)

targets =
  Enum.map(targets, fn i ->
    Nx.divide(Nx.tensor([i]), tmax)
  end)

:ok
```

Shuffle to disperse the species. the dataset has them in order 0-2

TODO Shuffle to disperse the species. Notice the dataset has them in order 0-2

TODO add this to the previous section on data prep; notice it and solve it there too

```elixir
dataset = Enum.zip(inputs, targets) |> Enum.shuffle()
ratio = 0.85
split = fn d -> Enum.split(d, ceil(count * ratio)) end

{train_set, test_set} = split.(dataset)
```

## Evaluate the algorithms

#### Define the Model

1. Epochs - how many training iterations to take over the dataset
2. Optimzer - function used to update weights and biases (hyperparameters)
3. Loss - how best to measure **loss** (distance between prediction & target)
4. Metric (optional) - choose metric to measure model's performance
5. Learning rate (optional) - size of the optimization step at each iteration

```elixir
epochs = 25
optimizer = :adam
loss = :mean_absolute_error

# the shape of our input: 1 row of 4 columns
input_shape = {nil, 4}
```

Define the model with the shape and a name for the input.

```elixir
input_name = "Iris Features"
model = Axon.input(input_shape, input_name)
```

Note the parameters at this point require 0 bytes of memory. That is because there are no layers to neural network. Before we add one, run the model in this state.

```elixir
model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

Notice the loss. It stays the roughly the same each epoch. This is because we aren't doing any actual learn.

Each layer in a neural network helps the network learn accurate representations of the input data. Without learning, the model performs badly.

<!-- livebook:{"break_markdown":true} -->

##### Add a layer

Pipe it afer the input and provide a `unit` to specify the number of output units. We'll start with 1.

```elixir
model =
  Axon.input(input_shape, input_name)
  |> Axon.dense(1, name: "First Layer")
```

Run it with the new layer.

```elixir
model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

Much improved! In this run, our loss dropped consistently from on epoch to the next.

Add another layer.

```elixir
model =
  Axon.input(input_shape, input_name)
  |> Axon.dense(5, name: "First Layer")
  |> Axon.dense(1, name: "Second Layer")
```

```elixir
model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

### Experiment with Metrics

Options: `:accuracy`, `:precision`, and `:recall`

```elixir
model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

### Experiment with Optimizers

Try: `:adam`, `:adamw`, `:adagrad`, and `:sgd`

```elixir
optimizer = :adamw

model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

### Experiment with Loss

We set the loss function to `:mean_absolute_error`.

Try setting it to `:mean_squared_error`.

```elixir
loss = :mean_squared_error

model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

### Experiement with Activation

Try: `:linear`, `:sigmoid`, `:tanh`, and `:relu`.

For this experiment we need to define our model. Try out different combinations of activation functions for each layer.

```elixir
first_layer_activation = :linear
second_layer_activation = :linear

model =
  Axon.input(input_shape, input_name)
  |> Axon.dense(5, activation: first_layer_activation, name: "First Layer")
  |> Axon.dense(1, activation: second_layer_activation, name: "Second Layer")

model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

## Enhance the Model

Now that we've adjusted Metrics, Loss, Optimizer, and Activation to find our best model, what more can we improve?

Experiment with all the variables. Add layers. Change layers' units. You can also assign a different learning rate. The default is `1.0e-3`.

We will also save the run to a variable so that when you find your best model, we can save it in a later block.

```elixir
epochs = 70

metric = :recall
loss = :mean_squared_error
first_layer_activation = :linear
second_layer_activation = :relu

# loss_rate = 0.001
# optimizer = Axon.Optimizers.adamw(loss_rate)

model =
  Axon.input(input_shape, input_name)
  |> Axon.dense(5, activation: first_layer_activation, name: "First Layer")
  |> Axon.dense(1, activation: second_layer_activation, name: "Second Layer")

model_state =
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(metric)
  |> Axon.Loop.run(train_set, %{}, epochs: epochs)
```

### Run and validate the model

```elixir
validated_model_state =
  model
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.validate(model, test_set)
  |> Axon.Loop.run(train_set, model_state, epochs: epochs)
```

## Save the model

```elixir
model_file = "elixir_conf/models/iris_nn_model.axon"

model
|> Axon.serialize(%{model_state: model_state})
|> then(&File.write!(model_file, &1))
```

```elixir
File.read!(model_file)
|> Axon.deserialize()
```

## Advanced

#### Dropout Rate

The rate at which to randomly drop out nodes during training to 
prevent overfitting and improve generalization error. It is a method for preventing a model from depending too much on specific neurons.

This example uses a 20% dropout rate.

```elixir
model =
  Axon.input(input_shape, input_name)
  |> Axon.dense(15, activation: second_layer_activation, name: "First Layer")
  |> Axon.dropout(rate: 0.2)
  |> Axon.dense(5, activation: first_layer_activation, name: "Second Layer")
  |> Axon.dropout(rate: 0.2)
  |> Axon.dense(1, activation: first_layer_activation, name: "Third Layer")
  |> Axon.Loop.trainer(loss, optimizer)
  |> Axon.Loop.metric(metric)
  |> Axon.Loop.run(train_set, %{}, epochs: 15)

:ok
```

#### Re-shape the targets

Try other loss functions require re-shaping the target.

* :binary_cross_entropy with from_logits: true
* :categorical_cross_entropy with from_logits: true

```elixir
loss =
  &Axon.Losses.binary_cross_entropy(
    &1,
    &2,
    from_logits: true
  )
```
