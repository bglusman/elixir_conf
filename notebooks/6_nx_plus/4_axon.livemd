# Nx / Axon

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:nx, "~> 0.2.1"}
])
```

## Goal

To provide an overview of the Axon library, which provides Nx-powered Neural Networks for Elixir. 

## Introduction

TODO Axon big picture

## Defining the Model

Axon enters our machine learning process when we have already preprocessed our data and are ready to train a model.

In the next chapter we will put Axon to use in our own Neural Network example. Here is a sneak peek of what that will look like.

```elixir
model =
  Axon.input({nil, 1, 28, 28}, "input")
  |> Axon.flatten()
  |> Axon.dense(128, activation: :relu)
  |> Axon.dense(10, activation: :softmax)
```

All `Axon` models start with an input layer to tell subsequent layers what shapes to expect. We then use `Axon.flatten/2` which flattens the previous layer by squeezing all dimensions but the first dimension into a single dimension. Our model consists of 2 fully connected layers with 128 and 10 units respectively. The first layer uses `:relu` activation which returns `max(0, input)` element-wise. The final layer uses `:softmax` activation to return a probability distribution over the 10 labels [0 - 9].

## Training

Axon boils the task of training down to defining a training step and passing the step to a training loop. You can use `Axon.Training.step/3` to create a generic training step with a model, a loss function, and an optimizer. In this example, we'll use *categorical cross-entropy* and the *Adam* optimizer. You can then pass this to a training loop with your training data to train the final model. `Axon.Training.train/4` lets you specify some additional options as well, such as the `Nx` compiler to use. 

Following is a non-functional example that trains for 10 epochs using the `EXLA` compiler. It will log Accuracy metrics every 100 training steps.

`model`
`|> Axon.Loop.trainer(:categorical_cross_entropy, :adam)`
`|> Axon.Loop.metric(:accuracy, "Accuracy")`
`|> Axon.Loop.run(data, %{}, epochs: 10, compiler: EXLA)`


## Prediction

Once you have the parameters from the training step, you can use them to make predictions using `Axon.predict`:

`Axon.predict(model, params, first_batch)`

## Concepts

Let's dig deeper into some of the Neural Networks concepts with Axon.

### Activation Functions

An activation function decides whether a neuron should be activated or not. This means that it will decide whether the neuronâ€™s input to the network is important or not in the process of prediction. The role of the Activation Function is to derive output from a set of input values fed to a layer.

The Activation Functions are typically divided into 2 types: linear and non-linear. A linear function graphs as a line; a non-linear graphs as a non-line or curve. Non-linear activation function are more popular because they make it easy for the model to generalize or adapt to a variety of data.

The activation function helps the neural network to use important data points and supress irrelevant ones. 

Axon provides options for activation: `:linear`, `:sigmoid`, `:tanh`, `:relu`, and more.

#### Linear

With linear activation function the activation is proportional to the input. The function doesn't do anything to the weighted sum of the input, it simply passes on the input it was given.

#### Sigmoid 

Maps logistic or multinomial regression output to probabilities, returning a value between 0 and 1.

#### Tanh 

Similar to sigmoid in shape, but with the advantage that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Its mainly used classification between two classes. Usually used in hidden layers of a neural network because it helps in centering the data and makes learning for the next layer much easier.

#### Relu 

ReLU (Rectified Linear Unit) is common in convolutional neural networks or deep learning. It allows for backpropagation while simultaneously making it computationally efficient. It does not activate all the neurons at the same time so is far more computationally efficient when compared to the sigmoid and tanh functions.

#### Softmax 

The softmax activation function takes in a vector of raw outputs of the neural network and returns a vector of probability scores. It is usually the last layer of a neural network.

### `dense` Layer

Axon's `dense` is a functional implementation of a dense layer. It performs linear transformation on the input. It takes these paramters in these shapes:

* `input` - `{batch_size, * input_features}`
* `kernel` - `{input_features, output_features}`
* `bias` - `{}` or `{output_features}`

Like this: `Axon.Layers.dense(input, kernel, bias)`

The input is transformed by computing the kernel matrix parameter and bias parameter:

`Nx.dot(input, kernel) + bias`

It returns `{batch_size, *, output_features}`

Since `kernel` and `bias` are typically learnable parameters, they are trained using a gradient method for optimization.

A gradient method is a type of algorithm to solve problems of form, in our class a slope. In this lesson we will see use a gradient descent method to determine the loss function.

Gradient descent is used to find the minimum value for a function. You begin with randam values for the parameters and then take small steps in the direction of the slope at each iteration.

### dropout

Dropout helps prevent overfitting. It does this by preventing the model from relying too much on certain connections. It takes a dropout `rate` to use to determine the probability `rate` and then scales the input tensor based on it.

## Next Steps or "what we don't have time to cover here"

#### Bilinear layers in Axon

Two input layers.

#### Convolutional & Pooling Layers in Axon

Axon provides a functional implementation of a general dimensional convolutional layer, which are commonly used in computer vision or when working with sequences and other input signals. Pooling is applied to the spatial dimension of the input tensor, often after convolutional layers.
