# Algorithms & Models

```elixir
Mix.install([
  {:httpoison, "~> 1.8"}
])
```

## Goal

This lesson will provide a brief introduction to some of the most common machine learning algorithms in order to understand why they are commonly used. We will also look at how the algorithms solve real-world problems.

## Overview

In the last section we learned that ML algorithms are basically procedures run on data, and they output models. We identified several well-known algorithms. Let's look at each of them a bit more closely to see how and why they are best put to use to solve particular problems.

## Supervised Learning Algorithms

Supervised Learning is the type of machine learning where the target outcome is known or labeled. Classification and Regression are examples of supervised learning algorithms. Classification is used when  the outcome is finite (a customer purchased an item). Regression is used when the outcome is infinite (the value of the purchase).

<!-- livebook:{"break_markdown":true} -->

![classification_regression](images/classification_regression.png)

<!-- livebook:{"break_markdown":true} -->

### Regression

Regression in data science and machine learning is a statistical method that enables predicting outcomes based on a set of input variables. The outcome is often a variable that depends on a combination of the input variables.

<!-- livebook:{"break_markdown":true} -->

#### Linear & Logistic Regression

Linear regression targets a prediction value based on independent variables to discover the relationship between variables and forecasting. Regression algorithms differ based on the relationship between dependent and independent variables and the number of independent variables. Linear regression seeks to plot a line of best fit calculated through the method of least squares.

Logistical regression estimates the probability of an event occurring based on a given dataset of independent variables. Its also used to estimate the relationship between a dependent variable and one or more independent variables, but it makes a prediction about a categorical variable versus a continuous one: true or false, yes or no, 1 or 0, etc.

Both linear and logistic regression are among the most popular algorithms within data science. Both are used to make predictions about future outcomes, linear regression is typically easier to understand. Linear regression also does not require as large of a sample size as logistic regression. Logistic regression needs a larger number of examples to represent values across all the response categories or the model may not have sufficient statistical power to detect a significant effect.

#### Uses

* fraud detection
* disease prediction
* binary classification problems like spam identification

<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #800080;height: 15.0px;" />

<span style="color: #800080;">

</span>

<!-- livebook:{"break_markdown":true} -->

<span style="color: #800080; font-size:50px; font-weight: bold; font-family: FreeMono, monospace">
Brain Break!
</span>

<!-- livebook:{"break_markdown":true} -->

<img src="https://static.thenounproject.com/png/506914-200.png" alt="Brain icon" style="width: 95px; float: left; margin-right: 40px;" />

Story Time. Let's use our imagination to understand what regression means and how it is useful.

Imagine you own a pet supply shop. First things first, your shop needs a name.

<!-- livebook:{"break_markdown":true} -->

Randomly select a pet type and call out to random word generator using HttPoison.

```elixir
pet = ["llama", "ferret", "parrot", "rabbit"] |> Enum.random()
url = "https://api.datamuse.com/words?rel_nry=#{pet}&max=50&md=p"
```

This returns a list of maps containing nearly rhyming words. Extract, transform, and load the adjectives from the list.

```elixir
adjs =
  HTTPoison.get!(url)
  |> case do
    %HTTPoison.Response{body: body} -> elem(Jason.decode(body), 1)
    _ -> []
  end
  |> Enum.map(fn w ->
    Enum.map(w, fn {k, v} -> {String.to_atom(k), v} end)
    |> Enum.into(%{})
  end)
  |> Enum.filter(fn w -> is_list(Map.get(w, :tags)) end)
  |> Enum.filter(fn w -> !String.contains?(w.word, " ") end)
  |> Enum.filter(fn w -> !Enum.count(w.tags) > 2 end)
  |> Enum.filter(fn w -> "adj" in Map.get(w, :tags) end)
  |> Enum.sort_by(& &1.score, :desc)
  |> Enum.sort_by(& &1.numSyllables, :desc)
  |> Enum.reverse()
  |> Enum.take(8)
```

Randomly select on and pair it with your random pet to craete your store name.

```elixir
pet = String.capitalize(pet)

adj =
  adjs
  |> Enum.random()
  |> Map.get(:word)
  |> String.capitalize()

store = Enum.join([adj, pet, "Pet Supply"], " ")
```

```elixir
IO.puts("-----------------PRESS RELEASE---------------------")
IO.puts("#{store} had best sales month ever!")
IO.puts("What will sales be next month?")
IO.puts("How can they continue the streak?")
IO.puts("Weather update: Will the rain ever stop?")
IO.puts("---------------------------------------------------")
```

![shop](images/petsupply.png)

<!-- livebook:{"break_markdown":true} -->

All eyes are on you. You need to predict next month’s sales number for your pet supply store. But how can anyone predict the future?

<!-- livebook:{"break_markdown":true} -->

### Variables

Dozens of factors from the weather to a competitor’s promotion to the newest pet food trend can impact the number. Your gut tells you the weather is the most important variable. You've experienced it yourself:

> When it rains, you sell more.

<!-- livebook:{"break_markdown":true} -->

### Regression Forcasts the Future

Regression can help determine which variable has the highest impact.

#### Dependent Variable

The dependent variable is the variable you want to predict. In this case, your dependent variable is monthly sales.

![supplies](images/putsupply3.png)

#### Independent Variables

These are the factors that **might** impact on your dependent variable.

<!-- livebook:{"break_markdown":true} -->

![](images/rain.png)

<!-- livebook:{"break_markdown":true} -->

### Data

So you gather your monthly sales numbers and the average monthly rainfall for the last three years. Plotted on a chart, the data looks like this:

![plot](images/shop_reg1.png)

The y-axis the dependent variable (sales) and the x-axis is the independent variable (rainfall).

It sure looks like your gut was right: the more rainful, the higher your sales.

<!-- livebook:{"break_markdown":true} -->

But how much does it have to rain to make a difference?

Put a line through the middle of the data points on our chart. This is an example of a __regression line__.

![regression_line](images/regression_line.png)

It is the line that best fits the data and explains the relationship between the independent variable and dependent variable.

<!-- livebook:{"break_markdown":true} -->

#### Formula

This formula explains the slope of the line: `y = 200 + 5x + error term`

`y = 200`

Historically when there was no rain, your sales averaged 200. Assuming other variables stay the same, you can expect future sales to be the same.

`5x`

Historically for every additional inch of rain you made an average of five more sales. For each increment `x` increases by one, `y` increases by five.

`error term`

It is tempting to stop here. Rain __clearly__ has the biggest impact on sales, but we need the `error term` to tell us whether rainfall is a variable __worth your attention__.

A regression line has an `error term` because independent variables are not perfect predictors of dependent variables. The line is an estimate. The larger the `error term`, the less confident you should be in your regression line. We could additional variables until the `error term` decreases.

```elixir
# rainfall inches
x = 7
y = 200 + 5 * x

IO.inspect(y, label: "predicted sales")

actual_sales = 245

error_term = actual_sales - y

IO.inspect(error_term, label: "error term")

y = 200 + 5 * x + error_term

IO.inspect(y, label: "predicted sales corrected with error term")

y == actual_sales
```

A major advantage of regression is the ability to discover the impact of multiple variables at once (Multiple Regression). For example, you could add data about promotions, competitors, and local pet events.

Regression get help determine which independent variable have the most influence over the depending variable. But remember, __correlation is not causation__.

<img src="https://imgs.xkcd.com/comics/correlation.png" alt="Correlation" />

<span style="font-size: 8px;">
[Image Source: xkcd](https://imgs.xkcd.com/comics/correlation.png)
</span>

We used regression to prove there is definitely a correlation between rain and monthly sales. We **cannot** confidently claim rain __caused__ the sales. We're selling pet supplies, not umbrellas after all.

<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #800080;height: 15.0px;" />

<span style="color: #800080;">

</span>

## Supervised Learning Algorithms

### Classification

In machine learning, classificaiton assigns an object as a member of a category or group. For example, classifiers are used to detect if an email is spam, or if a transaction is fraudulent.

<!-- livebook:{"break_markdown":true} -->

#### Naive Bayes

A naive byes model uses probability to classify objects based on features. It is a classification approach that employes the Bayes Theorem (principle of class conditional independence). The presence of one feature does not impact the presence of another in the probability of a given outcome.

#### Uses

* text classification
* spam detection
* recommendation engines

<!-- livebook:{"break_markdown":true} -->

#### Decision Trees & Random Forest

Decision trees are classifiers that determine which category an input belongs to by traversing the leaf’s and nodes of a tree. They are like a flow chart where the class of an object is determined step-by-step using certain known conditions.

A Random Forest is a collection of several decision trees from random subsets of the data. The result is a combination of trees that may make more accurate predictions than a single decision tree.

TODO insert binary tree image (I already have one somewhere -- I think it is for random forest but it could be cut)

#### Uses

* classification
* regression

<!-- livebook:{"break_markdown":true} -->

#### k Nearest Neighbors

K nearest neighbors (KNN) classifies data points based on their proximity and association to other available data to discover the most frequent or average characteristics shared among the objects. It assumes that similar data points can be found near each other and therefore calculates the distance between data points.

#### Uses

* problems with smaller datasets because the processing time grows as the size of the data grows
* recommendation engines
* image recognition

TODO: brain break needed here? data analysis task? The haberman dataset analysis for unbalanced dataset? Smartcell bar chart? Normalization function Boomer wrote? Or the fuel economy example from a few years ago.

<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #800080;height: 15.0px;" />

<span style="color: #800080;">

</span>

## Unsupervised Machine Learning Algorithms

Unsupervised learning algorithms do not have a supervisor. Their input data is typically unlabeled. They guide themselves through the training and find patterns and insights.

Unsupervised learning can perform more advanced processing jobs compared to supervised learning, however the results can be more irregular.

<!-- livebook:{"break_markdown":true} -->

### Clustering

<!-- livebook:{"break_markdown":true} -->

#### K-Means

The K-Means algorithm finds similarities between objects and groups them into K different clusters. If K=3, there will be three clusters, and for K=4, there will be four clusters. It splits the given unlabeled dataset into K clusters. Each dataset belongs to only one group that has related properties. Here is a basic walk-through:

1. assign a number to K
2. select arbitrary K points (centroids)
3. assign all data points to the nearest centroid (these are the clusters)

Then,

1. calculate the variance
2. choose new centroid for each cluster based on the variance
3. repeat until no new centroids are needed

<!-- livebook:{"break_markdown":true} -->

![](images/k-means.png)

<!-- livebook:{"break_markdown":true} -->

The K-means algorithm comes with a few difficulties:

1. they often make clusters of a similar size
2. a human as to chose the value of K and getting the optimal number can be difficult

#### Uses

* clustering IT alerts
* optimizing delivery of goods
* document clustering
* image segmentation
* indentifying location of police activity
* call record detail analysis

<!-- livebook:{"break_markdown":true} -->

#### Hierarchical Clustering

Hierarchical clustering builds a tree of nested clusters without having to specify the number of clusters. Hierarchical clustering starts with k = N clusters. It then merges the two closest into one cluster, obtaining k = N-1 clusters. The process of merging two clusters to obtain k-1 clusters is repeated until the desired number of clusters K is reached. Euclidean distance is used to determine which clusters to merge. Hierarchical clustering is deterministic, which means it is reproducible..

Nodes are compared with one another based on their similarity. Larger groups are built by joining groups of nodes based on their similarity. A criterion is introduced to compare nodes based on their relationship.Distance scores are defined between every pair of points. The clustering algorithm proceeds by iteratively identifying subclusters. The procedure can be top down or bottom up.

#### Uses

* biological data analysis
* social network data

<!-- livebook:{"break_markdown":true} -->

### Time Series

In a time-series model in machine learning the independent variables are a successive length of time: i.e. minutes, days, years. Those independent variables influce the the predicted variable. These models are often used to predict time-bound events like weather prediction. Time series forecasting can help to understand how historical data impacts the future.

Time series forecasting models are typically made of these four compenents:

1. trend - the increase or decrease in the series of data over time
2. seasonality - fluctuations in the pattern due to seasonal determinants over time
3. cyclical variations - when data shows irregular rise and fall intervals.
4. random variations - instability due to random factors that do not repeat through the pattern

#### Uses

* forecasting 
  * the spread of COVID-19
  * the daily generation of wind power
  * an avalanche in a famous ski resort

<!-- livebook:{"break_markdown":true} -->

### Deep Learning

A class of ML models that imitate more closely human information processing. The word "deep" comes from how the model is made up of many layers of processing to extract features from the input. Each layer of processing passes on a more abstract representation of the data to the next layer. Unlike the other models we've discussed, deep learning models don't need the input data to be labeled. They handle unstructured data and can tackle more human-like problems such as facial recognition or natural language processing.

<!-- livebook:{"break_markdown":true} -->

#### Neural networks

Neural networks are a type of deep learning. They process training data by closely modeling the interconnections of the human brain through layers of nodes, thus neural. The nodes are made up of inputs, weights, a bias, and an output. If that output value exceeds the bias (threshold), it activates the node and passes the data to the next layer. Neural networks learn their mapping function through supervised learning, adjusting based on the loss function through the process of gradient descent. Th model accuracy increases as the loss function nears zero and the liklihood of a correct prediction increases.

In the next section we will go into a more detailed look of deep learning and neural networks, including uses.

<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #800080;height: 15.0px;" />

<span style="color: #800080;">

</span>

## Choosing the Right Model

The best suited model depends on the outcome you want. For example, to predict the gas mileage of a vehicle from historical data, a good choice would be a supervised learning model like linear regression. The desired outcome is to predict the relationship between variables to forecast on outcome. Remember, linear regression was define above as:

> best for finding out the relationship between variables and forecasting

To identify if a potential customer in a particular city with a particular income and commute is likely to purchase a particular vehicle, a decision tree might be a better choice because they are:

> like a flow chart where the class of an object is determined step-by-step using certain known conditions

<!-- livebook:{"break_markdown":true} -->

### Model Deployment

Once we have trained a model we need to make it available for use in testing or production environment. The model may be integrated with other applications.
