# Neural Networks: Introduction

```elixir
Mix.install([
  {:req, "~> 0.3.0"},
  {:axon, "~> 0.2.0"},
  {:exla, "~> 0.3.0"},
  {:nx, "~> 0.3.0"}
])
```

## Goal

After introducing machine learning we’ll dive into Neural Networks which, themselves, are just another probabilistic method of learning about data. We’ll begin by first describing neural networks and then solving the problem of detecting numbers from images using the famous MNIST dataset.

<!-- livebook:{"break_markdown":true} -->

#### Big Picture

* Made up of layers of neurons that are the core processing units
  * Input layer (receives the input)
  * Hidden layers (perform most of the computations)
  * Output layer (predicts final output)
* Channels connect layers 
  * Each channel is assigned a numerical value: weight
  * Each *input* is multiplied by its weight and the result is sent as input to the *hidden layers*
  * Each of the neurons that receive that data have a corresponding numerical value called the *bias*
  * the bias is added to the *weight*
  * this is then sent to threshold function called the *activation function*
  * the result of this function will determine if the value will move on the network: *forward propagation*
  * the highest value at the end of the network is the *prediction*
  * the predicted *output* is compared to training data to learn how right or wrong the prediction is
    * this information is transferred backwards through the network: *back propagation*
    * the *weights* are adjusted

## Neural Networks

Neural networks are fundamentally composed of artificial neurons which are in turn modeled from biological neurons – cells in the brain that are excited by sensory input. For example the perceptron, the first artificial neuron, abstracts biological neurons by taking a set of weighted inputs (X1,X2,X3), processes them through an activation threshold (bias) and produces a binary output of 0 or 1.

![](images/neuron.png)

Therefore, a neural network is a collection of artificial neurons separated by “layers”. Each layer can process the input of another providing a rich feature set for prediction. Mathematically this can be described as “neural networks take an input vector of p variables X = (X1, X2, … Xp) and creates a nonlinear function f(X) to predict response Y”.

![](images/neural_network.png)

In modern applications of neural networks both the rectified linear unit (ReLU) and sigmoid activation functions are preferred over perceptrons. The key reason for further development of artificial neurons, and a key difference from perceptrons, is that modern activation functions output values between 0 and 1 while a perceptron’s output is binary, either 0 or 1. This makes tuning neural networks easier as small changes in the parameters won’t cause large changes throughout the network.

<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #800080;height: 5.0px;" />

<img src="https://static.thenounproject.com/png/2212696-200.png" alt="Brain icon" style="width: 85px; float: right; margin-right: 40px;" />

##### Definitions

* _Features_ - Input from data that we are interested in as predictors
* _Weights_ - Values attributed to features which indicates the importance as a predictor
* _Biases_ - A measure of how easy it is to get neuron to fire

<hr style="background-color: #800080;height: 5.0px;" />

## Brain Break!

<span style="color: #800080; font-size:30px; font-weight: bold; font-family: FreeMono, monospace">
Time for some Elixir
</span>

<img src="https://static.thenounproject.com/png/1111032-200.png" alt="Brain icon" style="width: 85px; float: left; margin-right: 40px;" />

Let's get some fingers on the keyboard and explore a simple neural network.

<!-- livebook:{"break_markdown":true} -->

Our first neural network will be the “hello, world” example of data science – a network built using the famous MNIST dataset. The MNIST dataset is a collection of handwritten digits and our goal is to correctly classify the digits 0-9. Let’s download the dataset and get a sense of what it looks like.

**Note**: We're using axon here to demonstrate a single neural network - don't worry about the implementation details of the library as we'll cover that in a later section.

```elixir
# Don't worry about learning this code - its just boilerplate to download our
# dataset
url = "https://storage.googleapis.com/cvdf-datasets/mnist/"
%{body: train_images} = Req.get!(url <> "train-images-idx3-ubyte.gz")
%{body: train_labels} = Req.get!(url <> "train-labels-idx1-ubyte.gz")

<<_::32, n_images::32, n_rows::32, n_cols::32, images::binary>> = train_images
<<_::32, n_labels::32, labels::binary>> = train_labels

images =
  images
  |> Nx.from_binary({:u, 8})
  |> Nx.reshape({n_images, 1, n_rows, n_cols}, names: [:images, :channels, :height, :width])
  |> Nx.divide(255)

# Visualization of the images so we can get a idea of what it looks like
images[[images: 0..4]] |> Nx.to_heatmap()
```

From running the above code we can see that the dataset is relatively simple. It contains images of handwritten numbers and labels corresponding to each image. Let’s go ahead feed this data into a single layer neural network.

```elixir
targets =
  labels
  |> Nx.from_binary({:u, 8})
  |> Nx.new_axis(-1)
  |> Nx.equal(Nx.tensor(Enum.to_list(0..9)))
  |> Nx.to_batched(32)

# Here is the core of our code - we're essentially taking out inputs,
# processing them through an activation function, and the output is our model
model =
  Axon.input("input", shape: {nil, 1, 28, 28})
  |> Axon.flatten()
  # |> Axon.dense(128, activation: :relu)
  |> Axon.dense(10, activation: :softmax)

images =
  images
  |> Nx.to_batched(32)

params =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, :adam)
  |> Axon.Loop.metric(:accuracy, "Accuracy")
  |> Axon.Loop.run(Stream.zip(images, targets), %{}, epochs: 5, compiler: EXLA)
```

As with most things in machine learning, our goal is to minimize the error (or the cost function) of our predictions. As we train our model on the MNIST dataset we can see that the accuracy improves while the error decreases. So, for a few lines of code, we're approximately 90% accuracy. Not bad! But can we do better?

## Multilayer Neural Networks

#### Weights

A weight is a value associated with a feature which indicates the importance of that feature in predicting the final value. The closer to zero a feature's weight, the less importance it has. Weight also tells us the relationship between a particular feature in the dataset and the target value.

Consider the example of predicting the likelihood of purchasing a vehicle with a dataset made of two input features: vehicle price and vehicle popularity.
Then consider the example as a line equation:

`w1 * (Vehicle Cost) + w2 (Vehicle Popularity) + b = 0`

When the vehicle cost goes up, the expression value goes up. If that expression value goes up, that means the likelihood of purchase goes up. That is the opposite of what we want!

This indicates to us that we need to invert the weight. The w1 value should be inverted from a positive to negative value. A negative weight value produces a negative product and the value of the expression decreases. That is the outcome we want: the higher the cost, the less the likelihood of purchase.

If a positive weight is associated with a feature there is a direct relationship between that feature and the target value. If a negative weight is associated with a feature there is an inverse relationship between the feature and the target value.

That is how weights help.

#### Biases

Biases are constant values added to the product of inputs and weights to offset the result. Without biases, a neural network is only doing matrix multiplication on inputs and weights. This leads to overfitting the data set.

By including biases we can reduce the variance and get better generalization. Biases shift the result of the activation function toward the left or right, the positive or negative.

## Conclusion

Neural Networks learn through the process of **Backpropagation**

```mermaid
graph TD;
  FP{{Forward Pass}}-->In([Input data]);
  In-->NN([Neural Network]);
  NN-->P[[Prediction]];
  P-->ME(Measure of Error);
  ME-- Backpropagation -->NN;
```

<!-- livebook:{"break_markdown":true} -->

[<span style="float: left; color: #800080; font-weight: bold; font-family: FreeMono, monospace">< previous chapter: Algorithms</span>](../3_Algorithms/4_Unsupervised.livemd)

[<span style="float: right; color: #800080; font-weight: bold; font-family: FreeMono, monospace">next chapter: Tools ></span>](../5_Tools/1_Intro.livemd)
