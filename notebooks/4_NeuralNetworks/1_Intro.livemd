# Neural Networks / Introduction

## Goal

After introducing machine learning we’ll dive into Neural Networks which, themselves, are just another probabilistic method of learning about data. We’ll begin by first describing neural networks and then solving the simple problem of X using Y dataset.

<!-- livebook:{"break_markdown":true} -->

#### Big Picture

* Made up of layers of neurons that are the core processing units
  * Input layer (receives the input)
  * Hidden layers (perform most of the computations)
  * Output layer (predicts final output)
* Channels connect layers 
  * Each channel is assigned a numerical value: weight
  * Each *input* is multiplied by its weight and the result is sent as input to the *hidden layers*
  * Each of the neurons that receive that data have a corresponding numerical value called the *bias*
  * the bias is added to the *weight*
  * this is then sent to threshold function called the *activation function*
  * the result of this function will determine if the value will move on the network: *forward propagation*
  * the highest value at the end of the network is the *prediction*
  * the predicted *output* is compared to training data to learn how right or wrong the prediction is
    * this information is transferred backwards through the network: *back propagation*
    * the *weights* are adjusted

## Neural Networks

### What are Neural Networks?

Neural networks are fundamentally composed of artificial neurons which are in turn modeled from biological neurons – cells in the brain that are excited by sensory input. For example the perceptron, the first artificial neuron, abstracts biological neurons by taking a set of weighted inputs (X1,X2,X3), processes them through an activation threshold (bias) and produces a binary output of 0 or 1.

![](images/neuron.png)

Therefore, a neural network is a collection of artificial neurons separated by “layers”. Each layer can process the input of another providing a rich feature set for prediction. Mathematically this can be described as “a neural networks take an input vector of p variables X = (X1, X2, … Xp) and creates a nonlinear function f(X) to predict response Y”.

![](images/Screen%20Shot%202022-08-20%20at%205.22.23%20PM.png)

In modern neural networks the preferred choice of activation function within a artificial neuron is the ReLU (rectified linear unit) activation function or, in earlier neural networks, sigmoid activation functions. The main reason for moving beyond perceptrons is that outputs of neurons were fixed at either 0 or 1. Modern neurons output values between 0 and 1 which provides smaller changes in the overall network when hyperparameter tuning.

### Application of Neural Networks

* Diagnosing and debugging systems
  * Categorize error logs (or other types of logs)
* Facial Recognition for age
  * Differential face from background
  * Correlate lines and spots on face that designate age
* Forecasting
  * rainfall or stock prices
* Music composition
  * based on patterns in existing functions

### TODO: Do we need this section? Should we add a definition section?

### Inputs, Weights, and Biases

An artificial neuron is a basic building block of the neural network. The  components are inputs, weights, and biases.

#### Inputs

Inputs are the set of values for which we need to predict the output value. They can be viewed as features or attributes in a dataset.

#### Weights

A weight is a value associated with a feature which indicates the importance of that feature in predicting the final value. The closer to zero a feature's weight, the less importance it has. Weight also tells us the relationship between a particular feature in the dataset and the target value.

Consider the example of predicting the likelihood of purchasing a vehicle with a dataset made of two input features: vehicle price and vehicle popularity.
Then consider the example as a line equation:

`w1 * (Vehicle Cost) + w2 (Vehicle Popularity) + b = 0`

When the vehicle cost goes up, the expression value goes up. If that expression value goes up, that means the likelihood of purchase goes up. That is the opposite of what we want!

This indicates to us that we need to invert the weight. The w1 value should be inverted from a positive to negative value. A negative weight value produces a negative product and the value of the expression decreases. That is the outcome we want: the higher the cost, the less the likelihood of purchase.

If a positive weight is associated with a feature there is a direct relationship between that feature and the target value. If a negative weight is associated with a feature there is an inverse relationship between the feature and the target value.

That is how weights help.

#### Biases

Biases are constant values added to the product of inputs and weights to offset the result. Without biases, a neural network is only doing matrix multiplication on inputs and weights. This leads to overfitting the data set.

By including biases we can reduce the variance and get better generalization. Biases shift the result of the activation function toward the left or right, the positive or negative.

<!-- livebook:{"break_markdown":true} -->

#### Deep Learning

Deep learning refers to extracting and transforming data through multiple layers of neural networks where each layer takes inputs from the previous layer and refines it progressively. Algorithms train the layers to minimize errors and increase accuracy.

## Conclusion

Neural Networks learn through the process of **Backpropagation**

```mermaid
graph TD;
  FP{{Forward Pass}}-->In([Input data]);
  In-->NN([Neural Network]);
  NN-->P[[Prediction]];
  P-->ME(Measure of Error);
  ME-- Backpropagation -->NN;
```
<!-- livebook:{"break_markdown":true} -->

[<span style="float: right; color: #800080; font-weight: bold; font-family: FreeMono, monospace">next ></span>](2_DeepLearning.livemd)
