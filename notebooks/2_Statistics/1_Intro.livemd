# Statistics Crash Course

## Goal

After going over machine learning we'll take a brief look at some underlying mathematical ideas.

### Statistical Learning

The Basics of Statistics

We hear a lot about machine learning being the subject of "allowing computers to learn without being explicity programmed". But what does that mean? Broadly speaking, it means given one or more inputs we want to build a model that predicts, or estimates, an output. To correctly predict a model uses probability and statistics to infer patterns and relationships from data – collectively this is known as statistical learning which encompasses a vast array of tools for *understanding data*.

### Examples

As humans we ourselves can infer ideas and concepts from data easily. If I mentioned what’s the probability of getting tails from flipping a fair coin, you would say 50%. If I asked you the chance the sun would rise tomorrow you would say 100% (at least, we hope so). However, humans are also very good at tricking themselves into assuming past events to continue into the future. In many cases this is an evolutionary necessity, but it also makes us very poor gamblers.

Let’s look at more complex scenarios:

Every week you play a lottery game where you choose 4 numbers between 1 to 80 and your friends give you the advice of:

```
a.  Chose two even and two odd numbers
b.  Calculate the average and pick 4 numbers not too far away
c.  Pick a high number, middle number, and low number
d.  Doesn’t matter which numbers you pick – it’s just luck
```

For example, you have flipped a fair coin 9 times and it has landed on tails 9 times in a row – what is the probability the next flip will be tails?

```
a. 10% (1/10)
b. 100%
c. 20%
d. 50%
```

(**Optional**) You’re a contestant on a game show and you’re presented with 3 doors – two of which have goats, one of which has a car. Let’s say you choose door 1. Before the gameshow host opens your selected door, they open door number 3, revealing a goat. They now give you the choice: keep your original choice or choose door number 2. What do you do? Why?

```
a.  Keep your original door
b.  Choose door number 2
```

<!-- livebook:{"break_markdown":true} -->

### Challenger Disaster

<!-- livebook:{"break_markdown":true} -->

The above set of examples demonstrate the counterintuitive world that can be demonstrated with statistical and probabilistic analysis. Although illustrative the problems exist in an abstract domain – so let’s look at the impacts of statistical analysis in the real world.

The Challenger disaster was a fatal space shuttle failure on January 28, 1986. The Space Shuttle Challenger broke apart in 73 seconds killing all crew members and lead to a large-scale investigation of NASA’s space program. In the end it was determined that the O-rings of the solid rocket booster failed due to cold weather temperatures. So how could NASA have known this was an issue? Well, let’s explore historical shuttle failure in cold weather with statistics.

![](images/oring_failure_nasa_commission.png)

The above image shows the number of incidents with shuttles in relation to the weather during the day of the launch. We can display the above data in a scatterplot to get a better sense of what we're dealing with.

![](images/oring_failure_scatterplot.png)

What does this data tell us? Well, we can see some cold weather incidents but really it doesn’t seem so dramatic. So, what happens if we apply some statistical methods – can we see more correlation with O-ring failure and cold weather? For this example, we’ll use a type of regression analysis called logistic regression.

Without getting too deep into the detail’s regression analysis tells us how much impact variables have on outcomes. In our case we want to know just how impactful weather is on O-ring failure. Further, we’re using logistic regression because we’re using qualitative data to represent solid rocket booster failure. If we were to use a quantitative method, like linear regression, it would assume there is a numerical importance of 1 = “failure” and 0 = “no failure”. But this is an arbitrary assignment with no numerical significance. Let’s see what plugging this data into a logistic regression model gets us.

![](images/oring_failure_logistic.png)

Here we have a clearer picture of O-ring – we can see a sharp increase of failure that is evident as the temperature decreases. Essentially for every one degree increase in temperature reduces the odds of failure by 0.84. At the time of the Challenger launch the temperature was 31 degrees F and while that’s well below our data we have on hand we can be confident that the failure rate would only increase. In fact, if they were to tell you they were launching in 31 degree weather with this dataset in front of you, your heart would skip a beat!
