# Nx in Action: Novice

```elixir
Mix.install([
  {:axon, "~> 0.1.0"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.2.1"},
  {:explorer, "~> 0.2.0"},
  {:kino, "~> 0.6.2"},
  {:kino_vega_lite, "~> 0.1.1"},
  {:req, "~> 0.3.0"},
  {:scidata, "~> 0.1.9"},
  {:vega_lite, "~> 0.1.4"}
])
```

## Question

Can we use Linear Regression to prediect passangers who will survive the Titanic disaster?

## Dataset

TODO

<span style="font-size: 14px;">
Citation: R. A. Fisher (1936) <span style="font-style: italic;">The use of multiple measurements in taxonomic problems. Annals of Eugenics.
</span>

![](images/titanic.png)

<!-- livebook:{"break_markdown":true} -->

### Features

TODO

* sepal_length `float`

### Observations

* There are 150 observations with 4 features each

## Load

With the use of Explorer, we can easily load the iris data into a dataframe.

```elixir
alias Explorer.DataFrame
alias Explorer.Series
```

```elixir
csv = "elixir_conf/data/titanic/train.csv"
IO.puts("Loading #{csv}")
df = DataFrame.from_csv!(csv)
```

```elixir
DataFrame.names(df)
```

```elixir
DataFrame.table(df)
```

### Observations

`Survived`

* 0 = No
* 1 = Yes

`Pclass` is a proxy for socio-economic status

* 1 = 1st Class = Upper Class
* 2 = 2nd Class = Middle Class
* 3 = 3rd Calss = Lower Class

`Age` is fractional if less than 1. If estimated, is it in the form of xx.5

`Embarked`

* C = Cherbourg
* Q = Queenstown
* S = Southampton

Family relations are defined as:

* Sibling (`SibSp`) = brother, sister, stepbrother, stepsister
* Spouse (`SibSp`) = husband, wife (mistresses and fiancÃ©s were ignored)
* Parent (`Parch`) = mother, father
* Child (`Parch`) = daughter, son, stepdaughter, stepson

```elixir
DataFrame.shape(df)
```

```elixir
age_values = df["Age"]
```

```elixir
age_values |> Series.nil?() |> Series.to_list() |> Enum.frequencies()
```

```elixir
# check all the columns for nils
DataFrame.names(df)
|> Enum.map(fn name ->
  df[name]
  |> Series.nil?()
  |> Series.count()
  |> DataFrame.to_columns()
  |> case do
    %{"values" => [false]} -> nil
    %{"counts" => [_, nils]} -> IO.inspect("#{name} has #{nils} nil values")
  end
end)

:ok
```

```elixir
# Dropping Columns which are not usefull
# Drop Name, Ticket, Cabin, Embarked, PassengerId
# Fill in nils for age 
df = DataFrame.select(df, ["Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare"])
```

```elixir
s =
  Series.transform(df["Sex"], fn v ->
    if v == "female", do: 0, else: 1
  end)

df = Explorer.DataFrame.mutate(df, Sex: s)
```

that gets rid of too much data
instead use `fill_missing`

we have options

##### Strategies

* :forward - replace nil with the previous value
* :backward - replace nil with the next value
* :max - replace nil with the series maximum
* :min - replace nil with the series minimum
* :mean - replace nil with the series mean

Ther are too many missing ages to go with mean; let's use the :forward to replace missing ages.

```elixir
df = Explorer.DataFrame.mutate(df, Age: df["Age"] |> Series.fill_missing(:forward))
```

But certain columns, like cabin, are too difficult to normalize (i.e. "C123") and seem unlike to influence accurancy. Remove it and PassengerID

```elixir
# Dropping Columns which are not usefull
# Drop Name, Ticket, Cabin, Embarked, PassengerId
# Fill in nils for age 
df = DataFrame.select(df, ["Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare"])
```

```elixir
s =
  Series.transform(df["Sex"], fn v ->
    if v == "female", do: 0, else: 1
  end)

df = Explorer.DataFrame.mutate(df, Sex: s)
```

```elixir
targets = DataFrame.select(df, &(&1 == "Survived"), :keep)
features = DataFrame.select(df, &(&1 == "Survived"), :drop)
```

```elixir
to_tensor = fn df ->
  df
  |> Explorer.DataFrame.names()
  |> Enum.map(&(Explorer.Series.to_tensor(df[&1]) |> Nx.new_axis(-1)))
  |> Nx.concatenate(axis: 1)
end

targets = to_tensor.(targets)
features = to_tensor.(features)
```

```elixir
batched_targets = Nx.to_batched_list(targets, 200)
batched_features = Nx.to_batched_list(features, 200)
batched = Stream.zip(batched_features, batched_targets)
```

```elixir
max = Nx.reduce_max(features, axes: [0], keep_axes: true)

normalize = fn {batch, target} ->
  {Nx.divide(batch, max), target}
end

training_set = batched |> Stream.map(&Nx.Defn.jit(normalize, [&1], compiler: EXLA))
```

```elixir
model =
  Axon.input({200, 6}, "input")
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.1)
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.1)
  |> Axon.dense(1)
  |> Axon.dense(3, activation: :softmax)
```

```elixir
epochs = 75

trained_model =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adamw(0.0001))
  # |> Axon.Loop.metric(:accuracy, "Accuracy")
  # |> Axon.Loop.metric(:precision)
  # |> Axon.Loop.metric(:recall)
  |> Axon.Loop.run(training_set, %{}, epochs: epochs)
```

```elixir
optimizer = Axon.Optimizers.adam(1.0e-3)

trained_model =
  model
  |> Axon.Loop.trainer(:binary_cross_entropy, :sgd)
  |> Axon.Loop.metric(:precision)
  |> Axon.Loop.metric(:recall)
  |> Axon.Loop.run(training_set, %{}, epochs: epochs)
```

```elixir
# inputs = features
# |> DataFrame.names()
# |> Enum.map(fn name ->
#   &(Series.to_tensor(features[&1])
#     |> Nx.new_axis(-1))
# end)
# |> Nx.concatenate(axis: 1)

max =
  features
  |> DataFrame.names()
  |> Enum.map(fn name ->
    Series.to_tensor(features[name])
    |> Nx.reduce_max(axes: [0], keep_axes: true)
  end)

normalize = fn {feature, target} ->
  IO.inspect(max, label: "max value of each feature")
  IO.inspect(feature, label: "max value of each feature")
  normalized = Nx.divide(feature, max)
  {normalized, target}
end

inputs = Stream.map(Stream.zip(features, targets), &Nx.Defn.jit(normalize, [&1]))

# inputs |> Nx.to_batched_list(30)

# train_df = Stream.zip(inputs, targets)
```

```elixir
model =
  Axon.input({nil, 1, 891, 7}, "input")
  |> Axon.conv(16, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.conv(32, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.dropout(rate: 0.1)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.conv(32, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.dropout(rate: 0.1)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.conv(32, kernel_size: {3, 3}, strides: 1, padding: :same, activation: :relu)
  |> Axon.dropout(rate: 0.2)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: 2, padding: :same)
  |> Axon.flatten()
  |> Axon.dense(16, activation: :relu)
  |> Axon.dropout(rate: 0.1)
  |> Axon.dense(8, activation: :relu)
  |> Axon.dense(2, activation: :softmax)
```

```elixir
epochs = 30

trained_model =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adamw(0.000005))
  # |> Axon.Loop.metric(:accuracy, "Accuracy")
  |> Axon.Loop.run(inputs, %{}, epochs: epochs)
```

```elixir
# normalize
# Sex: female = 0, male = 1
# Embarked: S = 0, C = 1, Q = 2
features
|> DataFrame.names()
|> Enum.map(fn name ->
  case name do
    "Sex" ->
      Series.transform(features[name], fn v ->
        if v == "female", do: 0, else: 1
      end)

    "Embarked" ->
      Series.transform(features[name], fn v ->
        if v == "S", do: 0, else: if(v == "C", do: 1, else: 2)
      end)

    _ ->
      s = Series.to_tensor(features[name])
      max = Nx.reduce_max(s, axes: [0], keep_axes: true)

      Series.transform(features[name], fn v ->
        Nx.divide(v, max)
        |> Nx.to_flat_list()
        |> List.first()
      end)
      |> Series.to_tensor()
      |> Nx.new_axis(-1)
  end
end)
|> Enum.map(
  &(Series.to_tensor(features[&1])
    |> Nx.new_axis(-1))
)
|> Nx.concatenate(axis: 1)

# inputs = Enum.map(features, fn feature ->  
#   Nx.divide(Nx.tensor(feature), feature_max)
# end)

# features = to_tensor.(features) |> IO.inspect(label: "features")
# targets = to_tensor.(targets) |> IO.inspect(label: "targets")

# s = Series.to_tensor(features[name])
# IO.inspect(name, label: "name")
# max = Nx.reduce_max(s, axes: [0], keep_axes: true)
# Series.transform(features[name], fn v ->
#   Nx.divide(v, max) |> IO.inspect(label: "n")
#   |> Nx.to_flat_list()
#   |> List.first()
# end)
# end)

# features = to_tensor.(features) |> IO.inspect(label: "features")
```

```elixir
feature_max =
  Nx.tensor(features)
  |> Nx.reduce_max(axes: [0], keep_axes: true)
```

```elixir
# Normalize Sex, Embark
# Embarked

# C = Cherbourg
# Q = Queenstown
# S = Southampton

features = DataFrame.select(df, &(&1 == "Survived"), :drop) |> IO.inspect(label: "features")
targets = DataFrame.select(df, &(&1 == "Survived"), :keep) |> IO.inspect(label: "targets")

feature_max =
  Nx.tensor(features)
  |> Nx.reduce_max(axes: [0], keep_axes: true)

# to_tensor = fn data_frame ->
#   data_frame
#   |> DataFrame.names()
#   |> Enum.map(
#     &(Series.to_tensor(data_frame[&1])
#       |> Nx.new_axis(-1))
#   )
#   |> Nx.concatenate(axis: 1)
# end

# inputs = Enum.map(features, fn feature ->  
#   Nx.divide(Nx.tensor(feature), feature_max)
# end)

# features = to_tensor.(features) |> IO.inspect(label: "features")
# targets = to_tensor.(targets) |> IO.inspect(label: "targets")

# feature_max =
# Nx.reduce_max(features, axes: [0], keep_axes: true)
# |> IO.inspect(label: "max value of each feature")

# batch = Stream.zip(features, targets)

# normalize = fn {batch, target} ->
#   normalized_feature = Nx.divide(batch, train_max)
#   {normalized_feature, target}
# end

# feature_max = 
#   Nx.tensor(features)
#   |> Nx.reduce_max(axes: [0], keep_axes: true)

# inputs = Enum.map(features, fn feature ->  
#   Nx.divide(Nx.tensor(feature), feature_max)
# end)
```

```elixir
train_df_inputs =
  DataFrame.select(train_df, &(&1 == "class"), :drop) |> IO.inspect(label: "features")

train_df_targets =
  DataFrame.select(train_df, &(&1 == "class"), :keep) |> IO.inspect(label: "targets")
```

```elixir
{features, targets} = Scidata.Iris.download()
```

```elixir
Enum.count(features) |> IO.inspect(label: "features")
Enum.count(features) |> IO.inspect(label: "targets")
:ok
```

```elixir
feature_max =
  Nx.tensor(features)
  |> Nx.reduce_max(axes: [0], keep_axes: true)
```

```elixir
inputs =
  Enum.map(features, fn feature ->
    Nx.divide(Nx.tensor(feature), feature_max)
  end)
```

```elixir
targets =
  Enum.map(targets, fn t ->
    Nx.tensor([t])
  end)
```

```elixir
data = Enum.zip([inputs, targets])
```

```elixir
set_size = Enum.count(data)
train_size = ceil(0.85 * set_size)
```

```elixir
data = Enum.shuffle(data)
train_set = Enum.slice(data, 0, train_size)
test_set = Enum.slice(data, train_size, set_size - train_size)
```

```elixir
scalar = fn tensor ->
  tensor |> Nx.to_flat_list() |> List.first()
end
```

```elixir
defmodule LinReg do
  import Nx.Defn

  @epochs 100
  @learning_rate 0.02

  defn predict({m, b}, x) do
    m * x + b
  end

  defn loss(params, x, y) do
    y_pred = predict(params, x)
    Nx.mean(Nx.power(y - y_pred, 2))
  end

  defn update({m, b} = params, inp, tar) do
    {grad_m, grad_b} = grad(params, &loss(&1, inp, tar))

    {
      m - grad_m * @learning_rate,
      b - grad_b * @learning_rate
    }
  end

  def train(data) do
    # match type & shape of inputs {4, 1} & targets {1}
    init_feature = Nx.random_normal({4, 1}, 0.0, 0.1)
    init_target = Nx.tensor([1], type: {:s, 64})
    init_params = {init_feature, init_target}

    for _ <- 1..@epochs, reduce: init_params do
      acc ->
        data
        |> Enum.reduce(
          acc,
          fn {x, y}, current_params ->
            update(current_params, x, y)
          end
        )
    end
  end
end
```

```elixir
model = LinReg.train(train_set)

test_set
|> Enum.each(fn {input, truth} ->
  prediction =
    LinReg.predict(model, input)
    |> scalar.()

  IO.inspect(
    "Actual: #{scalar.(truth)}. Predicted: #{prediction}. Loss: #{scalar.(truth) - prediction}"
  )
end)
```

```elixir
model = LinReg.train(data)

data
|> Enum.each(fn {input, truth} ->
  prediction =
    LinReg.predict(model, input)
    |> scalar.()

  IO.inspect(
    "Actual: #{scalar.(truth)}. Predicted: #{prediction}. Loss: #{scalar.(truth) - prediction}"
  )
end)
```

```elixir
import Nx.Defn

scalar = fn tensor ->
  tensor |> Nx.to_flat_list() |> List.first()
end

{features, targets} = Scidata.Iris.download()

feature_max = Nx.reduce_max(Nx.tensor(features), axes: [0], keep_axes: true)

inputs =
  Enum.map(features, fn feature ->
    Nx.divide(Nx.tensor(feature), feature_max)
  end)

targets =
  Enum.map(targets, fn t ->
    Nx.tensor([t])
  end)

data = Enum.zip([inputs, targets])

# match shape of inputs {4, 1}
init_feature = Nx.random_normal({4, 1}, 0.0, 0.1)
# match shape of targets {1, 1}
init_target = Nx.tensor([1], type: {:s, 64})

init_input = {init_feature, init_target}

model = LinReg.train(150, data, init_input)

# make some predictions
data
|> Enum.each(fn {input, truth} ->
  prediction =
    LinReg.predict(model, input)
    |> scalar.()

  IO.inspect(
    "Actual: #{scalar.(truth)}. Predicted: #{prediction}. Loss: #{scalar.(truth) - prediction}"
  )
end)
```

```elixir
scalar = fn tensor ->
  tensor |> Nx.to_flat_list() |> List.first()
end

target_m = :rand.normal(0.0, 10.0)
target_b = :rand.normal(0.0, 5.0)
target_fn = fn x -> target_m * x + target_b end

data =
  Stream.repeatedly(fn -> for _ <- 1..32, do: :rand.uniform() * 10 end)
  |> Stream.map(fn x -> Enum.zip(x, Enum.map(x, target_fn)) end)

IO.puts("Target m: #{target_m}\tTarget b: #{target_b}\n")
{m, b} = LinReg.train(100, data)
IO.puts("Learned m: #{scalar.(m)}\tLearned b: #{scalar.(b)}")
```

```elixir
import Nx.Defn

{features, targets} = Scidata.Iris.download()

feature_max = Nx.reduce_max(Nx.tensor(features), axes: [0], keep_axes: true)

inputs =
  Enum.map(features, fn feature ->
    Nx.divide(Nx.tensor(feature), feature_max)
  end)

targets =
  Enum.map(targets, fn t ->
    Nx.tensor([t], type: {:f, 32})
    |> Nx.reshape({1, 1})
  end)

data = Enum.zip([inputs, targets])

# match shape of inputs {4, 1}
weights = Nx.random_normal({4, 1}, 0.0, 0.1)
# match shape of targets {1, 1}
bias = Nx.random_normal({1, 1}, 0.0, 0.1)

wb = {weights, bias}

{m, b} =
  Enum.reduce(1..25, wb, fn epoch, params ->
    Enum.reduce(data, params, fn {input, target}, {weight, bias} ->
      LinReg.update({weight, bias}, input, target)
    end)
  end)

IO.puts("Learned m: #{LinReg.scalar(m)}\tLearned b: #{LinReg.scalar(b)}")
```

The Dataframe `shape/1` returns a tuple of shape of the data: `{rows, columns}`:

```elixir
alias Explorer.DataFrame
alias Explorer.Series

df = Explorer.Datasets.iris()

DataFrame.shape(df)
```

```elixir
DataFrame.n_rows(df)
```

```elixir
DataFrame.n_columns(df)
```

## Visualize

To visualize the data, use Explorer to sample a few rows of the Dataframe with `sample/3` and print it out in a table view with `table/2`

```elixir
df
|> DataFrame.sample(10)
|> DataFrame.table(limit: 3)
```

Verify there are 3 distict species of Irises in the dataset.

```elixir
df
|> DataFrame.distinct(columns: ["species"])
```

### Smart Cells

Another way to visualize the data is to use Livebook SmartCells. Set the data to the Iris dataframe and select an x-axis and y-axis.

Lets determine if our dataset is balanced. How many rows of each species are there? Evaluate the celll below to see.

<!-- livebook:{"break_markdown":true} -->

### Data Analytics

<!-- livebook:{"attrs":{"chart_title":null,"height":350,"layers":[{"chart_type":"bar","color_field":null,"color_field_aggregate":null,"color_field_type":null,"data_variable":"df","x_field":"species","x_field_aggregate":null,"x_field_type":"nominal","y_field":"__count__","y_field_aggregate":null,"y_field_type":null}],"vl_alias":"Elixir.VegaLite","width":750},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 750, height: 350)
|> VegaLite.data_from_values(df, only: ["species"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "species", type: :nominal)
|> VegaLite.encode(:y, aggregate: :count)
```

It reveals there are exactly 50 of each of the three Species features; we have a balanced dataset.

The x-axis captures the `species` column, which is a nominal type. The y-axis is set to `Count(*)`.

##### Types

* _Quantitative_ - numerical data
* _Nominal_ - categorical data without a set order or scale
* _Ordinal_ - categorial data with a set order or scale
* _Temporal_ - data which represents a state in time

<!-- livebook:{"break_markdown":true} -->

Change the x- and y-axis to experiment with visualizing other combinations. Here is a look at `petal_width` by `species` using color to indicate the count.

<!-- livebook:{"attrs":{"chart_title":null,"height":150,"layers":[{"chart_type":"bar","color_field":"__count__","color_field_aggregate":null,"color_field_type":null,"data_variable":"df","x_field":"species","x_field_aggregate":null,"x_field_type":"nominal","y_field":"petal_width","y_field_aggregate":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":750},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 750, height: 150)
|> VegaLite.data_from_values(df, only: ["species", "petal_width"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "species", type: :nominal)
|> VegaLite.encode_field(:y, "petal_width", type: :quantitative)
|> VegaLite.encode(:color, aggregate: :count)
```

## Training Set

Earlier we used `n_rows/1` to get the number of rows in the dataset. That is useful now to determine the size needed for training and testing.

The training set should have the majority of the data. Put 85% of the rows in this set. Its the data that âtrainsâ the machine learning model. The test set will be used to test the model to verify how accurately it makes predictions.

Split 85% of examples into a training set: `slice(df, offset, length)`

We will replicate these steps in the next section to create a test set with 15% of the rows.

```elixir
sample_size = DataFrame.n_rows(df)
train_size = ceil(0.85 * sample_size)
```

```elixir
train_df = DataFrame.slice(df, 0, train_size)
```

<hr style="background-color: #EED202;height: 15.0px;" />

<!-- livebook:{"break_markdown":true} -->

<span style="color: #DEA731; font-size: 1.7rem; font-weight: 600;">
<img src="https://static.thenounproject.com/png/1038260-200.png" alt="Caution icon" style="width: 50px; float: left; margin-top: 1px; margin-bottom: 10px; margin-right: 10px;" />CAUTION!
</span>

This is a novice excersize meant as a warm up to putting the Nx ecosystem into action. You'll note the Iris dataset is very small.

This method we are using is not suggestd for real world problems since splitting the dataset into a training and testing set reduces the size even more.

Since the goal is to predict iris species using a dataset the model has not seen before, ideally we would have a much larger training and testing set.

Remember back in an earlier lesson when we discussed the challenges of machine learning. Here we are at risk of **overfitting** the training data.

<!-- livebook:{"break_markdown":true} -->



<!-- livebook:{"break_markdown":true} -->

<hr style="background-color: #EED202;height: 15.0px;" />

<!-- livebook:{"break_markdown":true} -->



## Features & Targets

Next, split the testing set into:

1. input features
2. input targets

The target is the `species` column. The rest of the columns are features. Use Dataframe `select/3` to drop the `species` column to create a **feature set** and to keep only the `species` column for a **target set**.

```elixir
feature_set =
  train_df
  |> DataFrame.select(&(&1 == "species"), :drop)
```

```elixir
target_set =
  train_df
  |> DataFrame.select(&(&1 == "species"), :keep)
```

### Feature Normalization

<!-- livebook:{"break_markdown":true} -->

So far we've been working with Dataframes. It's time to convert to tensors.

It is also time to normalize the data.

For example, the features are `floats` such as 5.6. The values need to be normalized between 0 and 1.

First, get the features from the `feature_set`:

```elixir
features = DataFrame.names(feature_set)
```

Next, enumerate over that list. First we will just IO.inspect the list value on each iteration.

```elixir
features
|> Enum.map(fn name ->
  IO.inspect(name)
end)
```

Do the same enumeration again, only this time look at the feature's values.

```elixir
features
|> Enum.map(fn name ->
  IO.inspect(feature_set[name])
end)
```

They are each a Series. That makes it easy to convert them to tensors so we can find the maximum value of the feature. Like this:

```elixir
features
|> Enum.map(fn name ->
  s = Series.to_tensor(feature_set[name])
  Nx.reduce_max(s, axes: [0], keep_axes: true)
end)
```

Now that we have the maximum value for the feature in the enumeration, use it normalize the values.

Do do that, we want to transform the orignal Series into the value of the original value divided by the maximum value.

Nx division results in a tensor. We need a `float`. Convert the tensor to a flat list and take the only element.

```elixir
features
|> Enum.map(fn name ->
  s = Series.to_tensor(feature_set[name])
  max = Nx.reduce_max(s, axes: [0], keep_axes: true)

  Series.transform(feature_set[name], fn v ->
    Nx.divide(v, max)
    |> IO.inspect(label: "n")
    |> Nx.to_flat_list()
    |> List.first()
  end)
end)
```

The values are normalized!

Time to convert the transformed Series to a tensor.

```elixir
features
|> Enum.map(fn name ->
  s = Series.to_tensor(feature_set[name])
  max = Nx.reduce_max(s, axes: [0], keep_axes: true)

  Series.transform(feature_set[name], fn v ->
    Nx.divide(v, max)
    |> IO.inspect(label: "n")
    |> Nx.to_flat_list()
    |> List.first()
  end)
  |> Series.to_tensor()
end)
```

Pipe the tensor into `Nx.new_axis/3` and set the axis to negative, which will start from the back.

End the enumeration and concatenate all the tensors on a single axis.

```elixir
inputs =
  features
  |> Enum.map(fn name ->
    s = Series.to_tensor(feature_set[name])
    max = Nx.reduce_max(s, axes: [0], keep_axes: true)

    Series.transform(feature_set[name], fn v ->
      Nx.divide(v, max)
      |> IO.inspect(label: "n")
      |> Nx.to_flat_list()
      |> List.first()
    end)
    |> Series.to_tensor()
    |> Nx.new_axis(-1)
  end)
  |> Nx.concatenate(axis: 1)
```

```elixir
scalar = fn tensor ->
  tensor |> Nx.to_flat_list() |> List.first()
end

{features, targets} = Scidata.Iris.download()

feature_max = Nx.reduce_max(Nx.tensor(features), axes: [0], keep_axes: true)

inputs =
  Enum.map(features, fn feature ->
    Nx.divide(Nx.tensor(feature), feature_max)
  end)

targets =
  Enum.map(targets, fn t ->
    Nx.tensor([t], type: {:f, 32})
    |> Nx.reshape({1, 1})
  end)

data = Enum.zip([inputs, targets])

# match shape of inputs {4, 1}
init_feature = Nx.random_normal({4, 1}, 0.0, 0.1)
# match shape of targets {1, 1}
init_target = Nx.random_normal({1, 1}, 0.0, 0.1)

init_input = {init_feature, init_target}

model = LinReg.train(25, data, init_input)

# make some predictions
data
|> Enum.each(fn {input, truth} ->
  prediction =
    LinReg.predict(model, input)
    |> scalar.()

  IO.inspect("Actual: #{scalar.(truth)}. Predicted: #{prediction}")
end)
```

## Target Normalization

Our target are currently strings, i.e. "Iris-setosa". Normalize them by converting them to integers.

##### Label

* 0: Iris Setosa
* 1: Iris Versicolour
* 2: Iris Virginica

<!-- livebook:{"break_markdown":true} -->

Review what the `target_set` looks like:

```elixir
target_set
```

The "species" column is a Series of strings.

Access only that column, then transform the value such that each type gets its integer value.

```elixir
targets =
  target_set["species"]
  |> Series.transform(fn x ->
    case String.downcase(x) do
      "iris-setosa" -> 0
      "iris-versicolour" -> 1
      "iris-versicolor" -> 1
      "iris-virginica" -> 2
    end
  end)
```

Then convert the Series to a tensor.

```elixir
input_targets =
  targets
  |> Series.to_tensor()
  |> Nx.new_axis(-1)
```

## Train

### Classification Algorithm

We will train a multiclass classification model on this dataset.

It will map the input values (`x`) to a discrete output variable (`y`) and makes predictions by categorizing data into classes based on independent variables.

#### Outcome is CATEGORICAL: predicts a discrete (finite) class label.

##### Multiclass classification (more than two outcome labels)

* Iris species prediction  
  * setosa / versicolor /virginica

<!-- livebook:{"break_markdown":true} -->

### Define the Model

1. Epochs - how many training iterations to take over the dataset
2. Learning rate - size of the optimization step at each iteration
3. Loss - measure of the distance between the prediction from the label

```elixir
epochs = 20
learning_rate = 0.001
loss = :mean_squared_error
```

### Define Loss and Optimizer

First, the deaths in the dataset.

```elixir
import Nx.Defn
```

```elixir
# data = Enum.zip([inputs, input_targets])
# data = [inputs, targets]
# # data = Nx.concatenate([inputs, input_targets])
# Stream.zip(inputs, input_targets)
# |> Enum.map(fn x -> IO.inspect x end)
# |> Stream.map(fn {input, target} -> IO.inspect({input, target}) end)

# batch_inputs = Nx.to_batched_list(inputs, 10, leftover: :repeat)
# batch_targets = Nx.to_batched_list(input_targets, 10, leftover: :repeat)
# batch = Enum.zip(batch_inputs, batch_targets)

# batch = Enum.zip([inputs], [input_targets])

# data =
#   batch
#   |> Enum.map(fn {batch, target} -> 
#     {batch, target} 
#   end)

Nx.reduce([inputs, input_targets], 0, fn x, y ->
  IO.inspect(x, lable: "x")
  IO.inspect(y, label: "y")
end)
```

```elixir
weights = Nx.random_normal({3, 1}, 0.0, 0.1)
bias = Nx.random_normal({1, 1}, 0.0, 0.1)
wb = {weights, bias}
```

```elixir
Enum.reduce(1..epochs, wb, fn _epoch, params ->
  IO.inspect(data)
  # Enum.reduce(data, params, fn {input, target}, iteration_params ->
  #   # update(iteration_params, input, target)
  #   IO.inspect {iteration_params, input, target}
  # end)
end)
```

#### Loss

The loss function measures how well a model fit the data set.The bigger the difference between the prediction and the ground truth (target or class), the higher the loss function value.

```elixir
loss =
  &Axon.Losses.binary_cross_entropy(
    &1,
    &2,
    negative_weight: 1 / nondeaths,
    positive_weight: 1 / deaths,
    reduction: :mean
  )
```

Then the optimizer.

```elixir
optimizer = Axon.Optimizers.adam(0.01)
```

### Model

#### Train and run the model

When training, set the loss function and optimizer. When running, set the number of epochs.

##### Loss Function

The loss function measures how well a model fit the data set.The bigger the difference between the prediction and the ground truth (target or class), the higher the loss function value.

##### Optimizer

Implementations of common gradient-based optimization algorithms. Customize the Optimizer by passing in a learning rate. The learning rate is the amount the weights are updated during training. It is a hyperparameter. Use hyperparameters to fine tune your model.

##### Metric

Metrics are used to measure the performance and compare performance of models.

* Accuracy tells you how many times the model was correct overall
* Precision is how good the model is at predicting a specific category
* Recall tells you how many times the model was able to detect a specific category

##### Epochs

An epoch refers to one complete pass of the training data through the algorithm. It is  a hyperparameter. The maximum epochs to run the Axon loop for. Must be non-negative integer (default is 1).

```elixir
model_state =
  model
  |> Axon.Loop.trainer(:mean_squared_error, Axon.Optimizers.adamw(0.01))
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(training_data, %{}, epochs: 350)
```

### Save the Model

```elixir
model
|> Axon.serialize(%{model_state: model_state})
|> then(&File.write!("Projects/elixir_conf/models/simple_haberman_model.axon", &1))
```

### Testing

<!-- livebook:{"break_markdown":true} -->

todo description of the Axon evaluator, metrics, and run

<!-- livebook:{"break_markdown":true} -->

|                          | Positive (Actual) | Negative (Actual) |
| ------------------------ | ----------------- | ----------------- |
| **Positive (Predicted)** | True Positive     | False Positive    |
| **Negative (Predicted)** | True Negative     | False Negative    |

***

|                              | Truth: Yes hot dog! | Truth: No hot dog   |
| ---------------------------- | ------------------- | ------------------- |
| **Prediction: Yes hot dog!** | Hot Dog (food)      | Hot Dog (Dachshund) |
| **Prediction: No hot dog**   | Chili Dog (food)    | Pizza (food)        |

***

##### Haberman Dataset

|                                  | Truth: Survived | Truth: Did not surivive |
| -------------------------------- | --------------- | ----------------------- |
| **Prediction: Survived**         | true_positive   | false_positive          |
| **Prediction: Did not surivive** | false_negative  | true_negative           |

```elixir
{read_model, _params} =
  File.read!("Projects/elixir_conf/models/simple_haberman_model.axon")
  |> Axon.deserialize()

results =
  read_model
  |> Axon.Loop.evaluator()
  |> Axon.Loop.metric(:true_positives, "true_positive", :running_sum)
  |> Axon.Loop.metric(:true_negatives, "true_negative", :running_sum)
  |> Axon.Loop.metric(:false_positives, "false_positive", :running_sum)
  |> Axon.Loop.metric(:false_negatives, "false_negative", :running_sum)
  |> Axon.Loop.run(test_batch, model_state)
  |> Map.get(0)
  |> Enum.map(fn {k, v} ->
    %{
      "result" => k |> String.replace("_", " ") |> String.capitalize(),
      "count" => Nx.to_number(v)
    }
  end)

alias VegaLite, as: Vl

Vl.new(width: 500, height: 500)
|> Vl.data_from_values(results)
|> Vl.mark(:bar)
|> Vl.encode_field(:x, "result", type: :nominal, axis: [label_angle: 0])
|> Vl.encode_field(:y, "count", type: :quantitative)
```

## Predict

### TODO define prediction

```elixir
[prediction_batch | _] = train_batch_inputs

IO.inspect(prediction_batch)

{read_model, params} =
  File.read!("Projects/elixir_conf/models/simple_haberman_model.axon")
  |> Axon.deserialize()

# prediction_batch = Nx.to_batched_list(prediction_batch, 2)
Axon.predict(read_model, params, prediction_batch)
```

## Advanced Challenge

Can you increase the accuracy of your predictions using Multiple Regression by adding more variables, like the weight of the car?

The goal would be to model the CO2 emissions as a function of several car engines features.

You could even get a larger dataset with additional features [here](https://raw.githubusercontent.com/amercader/car-fuel-and-emissions/master/data.csv).
